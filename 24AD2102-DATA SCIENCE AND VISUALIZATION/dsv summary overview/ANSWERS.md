

CO1:

1.      Define data science or what is data science

2.      Data science is combination of -------------

Statistics

Machine learning

Visualization

Data management

Mathematical optimization

Social science

Law


---


3.      List few data science application

Fraud detection

Recommender systems

Healthcare

Education

Transportation

Agriculture

Finance

E-Business

Retail


---

4.      **Briefly discuss the roles in data science**
5.      **What are the roles in data science**
### 1. Data Scientist

**Responsibilities:**

- Analyze and interpret complex data sets to extract insights
    
- Build predictive models and machine learning algorithms
    
- Preprocess, clean, and engineer features
    
- Communicate findings to non-technical stakeholders
    

### 2. Machine Learning Engineer

**Responsibilities:**

- Develop and deploy machine learning models
    
- Optimize models for performance and scalability
    
- Collaborate with data scientists to operationalize work
    
- Manage and maintain model infrastructure
    

### 3. Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
    
- Create visualizations and reports for decision-making
    
- Identify trends and patterns in data
    
- Collaborate with business stakeholders to understand data needs
    

### 4. Data Engineer

**Responsibilities:**

- Build and maintain data pipelines and ETL processes
    
- Manage data infrastructure and databases
    
- Ensure data quality, reliability, and availability
    
- Support data scientists and analysts with clean, accessible data
    

### 5. Business Intelligence (BI) Analyst

**Responsibilities:**

- Create dashboards and reports for business performance tracking
    
- Design data visualization tools for end-users
    
- Identify key performance indicators (KPIs) and metrics
    
- Collaborate with business teams to support decision-making
    

### 6. Data Architect

**Responsibilities:**

- Design and maintain data architectures
    
- Define data storage, integration, and processing strategies
    
- Ensure data security and compliance
    
- Collaborate with data engineers to implement data solutions
    

### 7. Statistician

**Responsibilities:**

- Apply statistical techniques to analyze data
    
- Conduct hypothesis testing and experiments
    
- Design surveys and experiments to gather data
    
- Provide statistical insights to support decision-making
    

### 8. AI/ML Researcher

**Responsibilities:**

- Conduct research to advance machine learning and AI
    
- Develop novel algorithms and models
    
- Publish research papers and contribute to conferences
    
- Collaborate with data scientists and engineers on cutting-edge solutions
    

### 9. Quantitative Analyst (Quant)

**Responsibilities:**

- Apply quantitative and mathematical methods to financial data
    
- Develop trading strategies and risk models
    
- Analyze market data to inform investment decisions
    
- Implement algorithms for trading and risk management
    

### 10. Chief Data Officer (CDO)

**Responsibilities:**

- Set organizational data strategy and governance
    
- Oversee data management, privacy, and compliance
    
- Align data initiatives with business goals
    
- Manage data-related teams and resources

---

6.      Tools required for data science

7.      Data science tools

- **Programming Languages:** Python, R, SQL, Julia.
- **Data Handling & Analysis:** Pandas, NumPy, Spark, Hadoop.
- **Visualization:** Matplotlib, Seaborn, Tableau, Power BI, Plotly.
- **Machine Learning & AI:** Scikit-learn, TensorFlow, PyTorch, XGBoost, Keras.
- **Big Data & Cloud:** Hadoop, Spark, AWS, Azure, Google Cloud.
- **Databases:** MySQL, PostgreSQL, MongoDB, Cassandra.
- **Collaboration & Versioning:** Git, Jupyter Notebooks, VS Code.
- **ETL & Data Pipelines:** Apache Airflow, Talend, Luigi.

| Toolbox      | Purpose                        | Example Use Case                         |
| ------------ | ------------------------------ | ---------------------------------------- |
| NumPy        | Numerical computing            | Fast array operations, linear algebra    |
| Pandas       | Data manipulation and analysis | DataFrames for tabular data              |
| Matplotlib   | Data visualization (2D plots)  | Line graphs, bar charts, scatter plots   |
| Seaborn      | Statistical data visualization | Boxplots, correlation graphs             |
| Scikit-learn | Machine learning               | Classification, regression, clustering   |
| TensorFlow   | Deep learning                  | Neural networks and deep learning models |
| Statsmodels  | Statistical modeling           | Regression, hypothesis testing           |
| Plotly       | Interactive visualization      | Dashboards, real-time plots              |

9.      Discuss few data science sample case studies

**Fraud Detection**

- **Goal:** Identify fraudulent activity **before it causes major damage**.
- **Approach:** Analyze historical data to detect patterns and anomalies.
- **Challenges:** Real-time detection is **harder than post-fact**, requires **high precision**.
- **Trade-offs:** Both false positives (innocent flagged) and false negatives (fraud missed) are costly.
- **Tech:** Stream processing, anomaly detection models, real-time dashboards, ML algorithms.

---

**Recommender Systems:**

- **Purpose:** Deliver **personalized suggestions** to users.
- **Impact:** Boosts sales, click-throughs, conversions.
- **Examples:**
    - **Netflix:** ~$1B/year value from recommendations.
    - **Amazon:** 20–35% annual sales lift.
- **Technique:** Collaborative filtering at scale, content-based methods, hybrid approaches.

---

**Patient Readmission Prediction:**

- **Goal:** Identify why patients return to the hospital.
- **Benefits:** Cut costs, improve population health.
- **Focus:** Understand **underlying causes** for specific populations.
- **Data:** Integrate multiple sources—EHRs, socioeconomic info, genetics, patient history.
- **Approach:** Analyze correlations between readmissions and health/social factors to enable targeted interventions.

---

**Smart Cities:**

- **Definition:** Use data + ICT to optimize urban living.
- **Goals:**
    - Plan communities efficiently
    - Manage infrastructure/assets effectively
    - Reduce operational costs
    - Use open data to engage citizens
---

10.      Who is the responsible for designing dashboards

###  Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
- Create visualizations and reports for decision-making
- Identify trends and patterns in data
- Collaborate with business stakeholders to understand data needs

### Business Intelligence (BI) Analyst

**Responsibilities:**

- Create dashboards and reports for business performance tracking
- Design data visualization tools for end-users
- Identify key performance indicators (KPIs) and metrics
- Collaborate with business teams to support decision-making

---

**11.What is the primary goal of data science?**
answer c
a.      To build websites

b.      To create artistic visualizations

c.      To extract insights from dataInformation retrival

d.      To maintain servers

---


**12.What is the correct sequence of a typical Data Science lifecycle?**

answer b

a.      Data Cleaning → Data Collection → Modeling → Deployment

b.      Data Collection → Data Cleaning → Modeling → Deployment

c.      Deployment → Modeling → Data Collection → Data Cleaning

d.      Modeling → Data Collection → Deployment → Data Cleaning

---

**13.Discuss the responsibilities of Data analyst , Data Scientist and Data Engineer**

### Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
- Create visualizations and reports for decision-making
- Identify trends and patterns in data
- Collaborate with business stakeholders to understand data needs

### Data Engineer

**Responsibilities:**

- Build and maintain data pipelines and ETL processes
- Manage data infrastructure and databases
- Ensure data quality, reliability, and availability
- Support data scientists and analysts with clean, accessible data

### Data Scientist

**Responsibilities:**

- Analyze and interpret complex data sets to extract insights
- Build predictive models and machine learning algorithms
- Preprocess, clean, and engineer features
- Communicate findings to non-technical stakeholders

---

14.      Explain the typical life cycle of data science project

- **Problem Definition:** Clarify business/analytical objective.
- **Data Collection:** Gather data from internal and external sources.
- **Data Cleaning & Preprocessing:** Handle missing values, outliers, formatting, and normalization.
- **Exploratory Data Analysis (EDA):** Understand distributions, correlations, patterns, and anomalies.
- **Feature Engineering:** Create, select, or transform features to improve model performance.
- **Modeling:** Apply statistical or ML algorithms to build predictive or descriptive models.
- **Evaluation:** Assess model performance using metrics, cross-validation, and testing.
- **Deployment:** Integrate model into production or decision-making systems.
- **Monitoring & Maintenance:** Track performance, update models with new data, ensure reliability.

---

**15.Explain the following python libraries with example use case**

Numpy

Pandas

Scipy

Sikit-learn

Matplotlib


**NumPy** – Core numerical library for arrays and fast math.  
*Use case*: Vectorized matrix multiplication for ML.  
```python
import numpy as np
A = np.array([[1,2],[3,4]])
B = np.array([[5,6],[7,8]])
print(A @ B)  # matrix product
```


**Pandas** – Data analysis library with DataFrame for tabular data.  
*Use case*: Load CSV and filter rows.  

```python
import pandas as pd
df = pd.DataFrame({"Name":["A","B"],"Marks":[80,90]})
print(df[df["Marks"]>85])
```

**SciPy** – Scientific computing (stats, optimization, signal).  
*Use case*: Statistical test.  
```python
from scipy import stats
a = [1,2,3,4,5]
b = [2,3,4,5,6]
print(stats.ttest_ind(a, b))
```


**Scikit-learn** – ML toolkit (classification, regression, clustering).  
*Use case*: Train a classifier.  
```python
from sklearn.linear_model import LogisticRegression
X = [[0],[1],[2],[3]]
y = [0,0,1,1]
model = LogisticRegression().fit(X,y)
print(model.predict([[1.5]]))
```

**Matplotlib** – Plotting and visualization.  
*Use case*: Line chart.  
```python
import matplotlib.pyplot as plt
plt.plot([1,2,3],[2,4,6])
plt.xlabel("X")
plt.ylabel("Y")
plt.show()
```

**18. Integrate a function of one variable (0 to 1)**

```python
from scipy import integrate 
f = lambda x: 3*x**2 + 1 
result, _ = integrate.quad(f, 0, 1)
print("Definite integral:", result)
  ```

---

**19. Integrate a function of two variables (0 to 1 for both)**

```python
f = lambda x, y: 3*x**2 + 1 
result, _ = integrate.dblquad(f, 0, 1, lambda x: 0, lambda x: 1) print("Double integral:", result)```

---

**20/21. Supervised vs Unsupervised Learning**

- **Supervised Learning** – Labeled data, model learns input → output mapping.  
    _Example (Scikit-learn)_:
    

```python 
from sklearn.linear_model import LinearRegression 
X = [[1],[2],[3]] 
y = [2,4,6] model = LinearRegression().fit(X,y) print(model.predict([[4]]))```

- **Unsupervised Learning** – No labels, model finds patterns/clusters.  
    _Example (Scikit-learn)_:
    

```python
from sklearn.cluster import KMeans
X = [[1],[2],[10],[12]]
kmeans = KMeans(n_clusters=2).fit(X) 
print(kmeans.labels_)```

---

**22. Steps in building an ML model (Scikit-learn)**

1. Data collection
    
2. Data preprocessing (cleaning, scaling, encoding)
    
3. Split dataset (train/test)
    
4. Choose algorithm
    
5. Train model (`fit`)
    
6. Evaluate model (`predict` + metrics)
    
7. Tune hyperparameters
    
8. Deploy / use for predictions
    

---

**23. Classification model evaluation metrics**

- Accuracy, Precision, Recall, F1-score
    
- Confusion matrix
    
- ROC-AUC curve
    
- Log loss
    

---

**24. Define Data Analytics**  
Extraction, transformation, and analysis of data to generate insights and support decision-making.

---

**25. Pip and Conda**

- **Pip** – Python’s standard package installer (`pip install package`).
    
- **Conda** – Environment and package manager (Python + non-Python packages) for reproducibility.
    

---

**26. Python package management tools**

- **Pip** – Installs packages from PyPI.
    
- **Conda** – Manages environments and packages; works for Python + other binaries.
    
- **Miniconda** – Lightweight Conda installer; minimal base setup for creating environments.


**27.How many ways you can launch jupyter notebook**

- Terminal / Command Prompt: `jupyter notebook`
    
- Anaconda Navigator → Launch Jupyter Notebook
    
- VS Code → Open Jupyter Notebook

**28.Write a command to install new python libraries on terminal or anaconda power shell**

- Using pip:
    

`pip install package_name`

- Using conda:
    

`conda install package_name`

**29.Write a steps to create your own virtual environment(venv)**

1. Open terminal / CMD
    
2. Create env:
    

`python -m venv myenv`

3. Activate env:
    

- Windows: `myenv\Scripts\activate` 

4. Install packages within this environment
    
5. Deactivate when done: `deactivate`


**30.How to update a package using conda**

conda update package_name

**31.Write a code to display the version of package**


- Using pip:
    

`pip show package_name`

- Using Python:
    

`import package_name print(package_name.__version__)`

- Using conda:
    

`conda list package_name`


---


**32.What are types of data**

a.      Record Data

b.      Transaction Data

c.      Graph Data (Network Data)

d.      Spatial Data

e.      Time-Series Data

f.       Text Data

g.      Sequence Data

h.      Hierarchical Data

### Record Data

A collection of records, each with the same set of attributes.

- **Examples:** Relational databases, Excel spreadsheets
- **Format:** Rows = objects, Columns = attributes

**Example Table:**

| Name  | Age | Dept |
| ----- | --- | ---- |
| Alice | 20  | CSE  |
| Bob   | 22  | ECE  |

### Transaction Data

A set of transactions where each transaction is a set of items.

- **Examples:** Market basket data, Online purchase logs

**Example Table:**

| TID | Items               |
| --- | ------------------- |
| 1   | Milk, Bread, Butter |
| 2   | Bread, Eggs         |
| 3   | Milk, Eggs, Diaper  |

### Graph Data (Network Data)

Consists of **nodes** (objects) and **edges** (relationships).

- **Used for:** Social networks, web analysis, communication networks
- **Example (Social Network):**
    - Nodes: People (e.g., Alice, Bob, Carol)
    - Edges: Friendships (e.g., Alice ↔ Bob, Bob ↔ Carol)

### Spatial Data

Objects have **spatial locations** (coordinates).

- **Use Case:** Google Maps, satellite imagery, urban planning
- **Used for:** Maps, GPS, GIS (Geographic Information Systems)
- **Example:**
    - A point: `(latitude, longitude)` → `(17.385, 78.4867)`
    - A region: polygon boundaries of a city

#### Categorical Data

**Definition:** Data that represents categories or groups.  
**Characteristics:**

- No inherent numeric meaning
- Arithmetic operations are not meaningful  
    **Examples:**
- Gender (Male, Female)
- Color (Red, Blue, Green)
- Department (HR, Sales, IT)

#### Numerical Data

**Definition:** Data that consists of numbers and can be measured.  
**Types:**

- **Discrete:** Countable values (e.g., number of children)
- **Continuous:** Measurable quantities (e.g., height, weight)  
    **Examples:**
- Age = 25
- Salary = $50,000

#### Ordinal Data

**Definition:** Categorical data with a clear ordering or ranking.  
**Characteristics:**

- Order matters, but the difference between values is not meaningful  
    **Examples:**
- T-shirt size (Small < Medium < Large)
- Customer satisfaction (Poor < Fair < Good < Excellent)

#### Binary Data

**Definition:** Data with only two possible values.  
**Types:**

- **Symmetric:** Both outcomes are equally important (e.g., Male/Female)
- **Asymmetric:** One outcome is more important (e.g., Disease: Yes/No)  
    **Examples:**
- Yes/No
- True/False

---

**33.What is nominal type attribute and explain with example**

#### Nominal Attributes

**Definition:** Names or labels with no ordering  
**Operations:** Equality comparison  
**Examples:** Colors, ZIP codes

---
**34.What is ordinal type attribute and explain with example**

#### Ordinal Attributes

**Definition:** Ordered categories  
**Operations:** Comparisons such as `<`, `>`  
**Examples:** Rankings, grades

---
**35.Differentiate Discrete and continuous attributes with examples**

|Attribute Type|Definition|Example|
|---|---|---|
|**Discrete**|Takes countable, separate values|Number of students in a class: 20, 25, 30|
|**Continuous**|Can take any value in a range (measurable)|Height of students: 150.5 cm, 162.3 cm, 170.0 cm|

---

**36.List qualitative attributes**

- **Nominal** (no natural order):
    
    - Color (Red, Blue, Green)
        
    - Gender (Male, Female, Other)
        
    - Blood Group (A, B, AB, O)
        
    - Nationality (Indian, American, etc.)
        
    - Product Type (Electronics, Furniture)
        
- **Ordinal** (has natural order):
    
    - Education Level (High School < Bachelor < Master < PhD)
        
    - Customer Satisfaction (Poor < Average < Good < Excellent)
        
    - Military Rank (Private < Sergeant < Captain < General)
        

---

**37.List quantitative attributes**

- **Discrete** (countable values):
    
    - Number of siblings
        
    - Number of students in a class
        
    - Number of cars owned
        
- **Continuous** (measurable values, can take any value in a range):
    
    - Height (cm)
        
    - Weight (kg)
        
    - Salary (₹)
        
    - Temperature (°C)

---

**38.Researcher wants to analyze the relationship between student performance and hours studied. What type of dataset would they likely use? What kind of analysis techniques would be suitable?**

**Dataset Type:** Quantitative (numerical) dataset with at least two variables:

- Independent variable: Hours studied (continuous)
    
- Dependent variable: Student performance/marks (continuous)
    

**Analysis Techniques:**

- **Correlation Analysis:** To measure strength and direction of relationship.
    
- **Regression Analysis:**
    
    - Simple Linear Regression: Predict performance from hours studied.
        
    - Multiple Regression (if including more factors like attendance, sleep).
        
- **Visualization:** Scatter plots to observe trends and patterns.

**39.Discuss how to Measure Data Similarity and Dissimilarity**
#### Similarity

Numerical measure of how alike two data objects are.

- Value is higher when objects are more alike.
- Often falls in the range ([0, 1]).

**What it is:** A number that tells us how much two data points are alike.

- Higher value → more alike.
- Usually ranges from 0 (not alike) to 1 (exactly alike).

#### Dissimilarity (e.g., distance)

Numerical measure of how different two data objects are.

- Lower when objects are more alike.
- Minimum dissimilarity is often 0.
- Upper limit varies.

**What it is:** A number that tells us how different two data points are.

- Lower value → more alike.
- Minimum = 0 (identical points).
- Maximum can vary.

**Proximity** refers to a similarity or dissimilarity. **Proximity** is just a general term for either similarity or dissimilarity.


they can be calculated by using 

- Euclidean distance formula:

$$
d(x_i, x_j) = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \dots + (x_{ip}-x_{jp})^2}
$$

- $x_i, x_j$ = objects  
- $p$ = number of attributes  
- Dissimilarity matrix is **symmetric**: $d(x_i, x_j) = d(x_j, x_i)$  
- Useful for clustering or similarity analysis

**40.What is symmetric and asymmetric binary variables**

#####  Symmetric vs Asymmetric Binary Attributes

|Attribute Type|Definition|Count 0?|Example|
|---|---|---|---|
|Symmetric|Both outcomes are equally meaningful|Yes|Gender (M/F), Yes/No survey response|
|Asymmetric|Only one outcome is meaningful; 0 carries little/no info|No (ignore 0-0 matches)|Fever presence, Positive test result|

**Rule of Thumb:**

- Ask: “Does absence (0) carry information?”
    - Yes → Symmetric
    - No → Asymmetric

---

**41.What is the distance measure for symmetric binary variable**

#### Distance Measure for Symmetric Binary Variables

For symmetric binary variables, both 1s and 0s matter equally. The distance between objects i and j is:

Explanation:

- **Numerator (r + s):** Number of mismatches between i and j
- **Denominator (q + r + s + t):** Total number of attributes

> Higher distance → more dissimilar objects.  
> Lower distance → more similar objects.

Where:

- **q** = number of attributes where both i and j are 1
- **r** = number of attributes where i = 1, j = 0
- **s** = number of attributes where i = 0, j = 1
- **t** = number of attributes where both are 0
- **p** = total number of attributes

**42.What is the distance measure for asymmetric binary variable**

#### Jaccard Coefficient (Similarity Measure for Asymmetric Binary Variables)

The **Jaccard coefficient** measures similarity rather than distance:

Explanation:

- **Numerator (q):** Number of attributes where both i and j are 1
- **Denominator (q + r + s):** Attributes where at least one object has 1

> Value ranges from 0 (completely different) to 1 (identical).

Where:

- **q** = number of attributes where both i and j are 1
- **r** = number of attributes where i = 1, j = 0
- **s** = number of attributes where i = 0, j = 1
- **t** = number of attributes where both are 0
- **p** = total number of attributes


Calculate the distance between asymmetric binary variables between (jack,mary) and (jack,jim) and what is your inference with the output

---

**43.Calculate the distance between asymmetric binary variables between (jack,mary) and (jack,jim) and what is your inference with the output**

| Name | Gender | Fever | Cough | Test-1 | Test-2 | Test-3 | Test-4 |
| ---- | ------ | ----- | ----- | ------ | ------ | ------ | ------ |
| Jack | M      | Y     | N     | P      | N      | N      | N      |
| Mary | F      | Y     | N     | P      | N      | P      | N      |
| Jim  | M      | Y     | P     | N      | N      | N      | N      |

44.**Calculate the similarity distance between given points using Euclidean, Manhattan, and Supremum distance measures and draw conclusions.**

![[Pasted image 20250829112414.png]]


---

**45.What is cosine similarity and explain with case study**

#### Cosine Similarity

If **d1** and **d2** are two vectors (e.g., term-frequency vectors), then:
$$
cos(d1,d2)=d1⋅d2∥d1∥∥d2∥\text{cos}(d_1, d_2) = \frac{d_1 \cdot d_2}{\|d_1\| \|d_2\|}
$$

Where:

- *⋅* indicates the vector dot product
    
- ∥d∥ is the length (magnitude) of vector **d**
    

####  Example

##### Term-Frequency Table

|Document|teamcoach|hockey|baseball|soccer|penalty|score|win|loss|season|
|---|---|---|---|---|---|---|---|---|---|
|Document|5|0|3|0|2|0|0|2|0|
|Document2|3|0|2|0|1|1|0|1|0|

##### Cosine Similarity Calculation Example

###### Vectors
$$
d1=[5,0,3,0,2,0,0,2,0],d2=[3,0,2,0,1,1,0,1,0]
$$
###### Dot Product
$$
d1⋅d2=5∗3+0∗0+3∗2+0∗0+2∗1+0∗1+0∗0+2∗1+0∗0=25 
$$
###### Magnitudes

$$
\|d_1\| = \sqrt{5^2 + 0^2 + 3^2 + 0^2 + 2^2 + 0^2 + 0^2 + 2^2 + 0^2} = \sqrt{42} \approx 6.48 $$

$$\|d_2\| = \sqrt{3^2 + 0^2 + 2^2 + 0^2 + 1^2 + 1^2 + 0^2 + 1^2 + 0^2} = \sqrt{16} = 4
$$
###### Cosine Similarity
$$
\cos(d_1, d_2) = \frac{25}{6.48 * 4} \approx 0.964
$$
###### Interpretation

The two documents are highly similar (~0.96 cosine similarity).

---

**46.Distinguish between symmetric and asymmetric binary variables when measuring similarity.**

#####  Symmetric vs Asymmetric Binary Attributes

|Attribute Type|Definition|Count 0?|Example|
|---|---|---|---|
|Symmetric|Both outcomes are equally meaningful|Yes|Gender (M/F), Yes/No survey response|
|Asymmetric|Only one outcome is meaningful; 0 carries little/no info|No (ignore 0-0 matches)|Fever presence, Positive test result|

**Rule of Thumb:**

- Ask: “Does absence (0) carry information?”
    - Yes → Symmetric
    - No → Asymmetric
---

**49.What is the function to  list of all the variable and function names in the module.**

answer dir

      Dir()

      Head()

      Tail()

      Describe()
---      
 **50.Write a python code for displaying single dimensional array using for loop**

```python
import numpy as np

# Create a 1D array
arr = np.array([10, 20, 30, 40, 50])

# Display elements using for loop
for element in arr:
    print(element)
```
---
51.Write a python code for sorting given array [65,34,67,10,45,2]

```python
arr = [65, 34, 67, 10, 45, 2]

# Sort the array
arr.sort()

# Display sorted array
print(arr)
```
---
**52.Declare an array with elements 1..10 and write code for displaying even numbers**

```python
import numpy as np

# Declare array with elements 1 to 10
arr = np.arange(1, 11)

# Display even numbers
for num in arr:
    if num % 2 == 0:
        print(num)
```
---
**53.Declare an array with 9 elements and write a code to display elements from 3 to 7**
```python
import numpy as np

# Declare array with 9 elements
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])

# Display elements from index 2 to 6 (3rd to 7th element)
for num in arr[2:7]:
    print(num)
```

---
**54.How to check the array is empty or not**
```python
import numpy as np

# Example arrays
arr1 = np.array([])
arr2 = np.array([1, 2, 3])

# Check if empty
print(arr1.size == 0)  # True
print(arr2.size == 0)  # False
```

---
**55.What is ndim attribute**

```python
import numpy as np

arr = np.array([[1, 2, 3], [4, 5, 6]])
print(arr.ndim)  # Output: 2
```
---

**56.What is shape and reshape of an array with examples**

```python
import numpy as np

# Original array
arr = np.array([1, 2, 3, 4, 5, 6])
print(arr.shape)  # Output: (6,)

# Reshape to 2 rows and 3 columns
arr2 = arr.reshape(2, 3)
print(arr2)
print(arr2.shape)  # Output: (2, 3)
```

---

**57.Write a code to create 5X5 identity matrix**
```python
import numpy as np

# Create 5x5 identity matrix
identity_matrix = np.eye(5)

print(identity_matrix)
```

**58.What is linespace() in numpy**

`linspace()` in NumPy generates **evenly spaced numbers over a specified interval**.

Syntax:

`numpy.linspace(start, stop, num=50, endpoint=True)`

- `start` – starting value
    
- `stop` – ending value
    
- `num` – number of values to generate (default 50)
    
- `endpoint` – if True, includes `stop`
    

Example:

`import numpy as np  arr = np.linspace(0, 10, 5) print(arr)`

---

**59.Write a code to create diagonal matrix**

```python
import numpy as np

# Elements for the diagonal
diagonal_elements = [1, 2, 3, 4, 5]

# Create diagonal matrix
diag_matrix = np.diag(diagonal_elements)

print(diag_matrix)

```
---
  **60.What is the method for calculating sum of diagonal elements in a matrix**

```python
import numpy as np

matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

diag_sum = np.trace(matrix)
print(diag_sum)  # Output: 15
```
---
**61.Discuss the possibilities to transpose a matrix using numpy**

In NumPy, a matrix can be transposed in multiple ways:

1. **`.T` attribute** – simplest:
    

```python
import numpy as np
mat = np.array([[1, 2], [3, 4]])
print(mat.T)
```

2. **`np.transpose()` function** – general, can reorder axes for higher dimensions:
    

```python
print(np.transpose(mat))
```

3. **`swapaxes()`** – swap two specified axes (useful in >2D arrays):
    

```python
print(mat.swapaxes(0, 1))
```

All produce the same result for 2D arrays:

```
[[1 3]
 [2 4]]
```
---
**62.Discuss the accessing elements with indexes

1. **1D array** – single index:
    

```python
import numpy as np arr = np.array([10, 20, 30, 40]) print(arr[2])  # Output: 30
```

2. **2D array** – row and column indices:
    

```python
mat = np.array([[1, 2], [3, 4]]) print(mat[1, 0])  # Output: 3 (row 1, column 0)
```

3. **Negative indexing** – count from end:
    

```python
print(arr[-1])  # Output: 40
```

4. **Slicing** – access ranges:
    

```python
print(arr[1:3])  # Output: [20 30] 
print(mat[:, 1])  # Output: [2 4] (all rows, column 1)
```

Indexes can be **single integers, slices, or lists/arrays** for advanced selection.


CO2:

1.      Define structured data and give one example.

2.      What is the role of data cleaning in data preprocessing?

3.      Mention any two data collection strategies in the context of data science and visualization.

4.      What is personally identifiable information (PII) and why is it important in data privacy?

5.      Name any two techniques used to ensure data integrity.

6.      What is the main purpose of data masking?

7.      Give one difference between structured and semi-structured data with examples.

8.      Explain the three types of data sources—structured, unstructured, and semi-structured—with examples, and discuss their implications in data preprocessing.

9.     Describe in detail the various data collection strategies used in data science and visualization, highlighting how each contributes to quality insights.

10.  Discuss the different data security issues mentioned in the session and explain methods to address each.

11.  Explain the complete process of data preprocessing, including cleaning, integration, transformation, reduction, and discretization, with examples.

12.  What is data encryption? Explain the importance of encryption in transit and at rest with suitable examples.

13.  Discuss the importance of data privacy in analytics and visualization and explain how anonymization and differential privacy help maintain it.

14.  Describe the concept of compliance with regulations in data handling and explain the consequences of non-compliance with examples such as GDPR and HIPAA.

15.  Define noise,incomplete, inconsistent

16.  What are the measures to asses the data quality

17.  Explain about data preprocessing tasks

18.  What are the causes for missing data?

19.  How to handle missing data?

20.  Explain the scenario for conversion of nominal to numeric?

21.  List the techniques to handle missing data

22.  Explain Binning methods for smoothing data with example?

23.  What outlier analysis?

24.  What are the causes for occurring inconsistent data with examples?

25.  Which of the following is a common method for handling missing data in a dataset?

A. Data normalization  
B. Data encryption  
C. **Data imputation  
**D. Data transformation

26.  Which technique is used to detect and correct noisy data?

A. Data visualization  
B. **Smoothing techniques**  
C. Data mining  
D. Hashing

27.  Which of the following tools or techniques is typically used to identify duplicate records in a dataset?

A. Data labeling  
B. **String matching or record linkage  
**C. Data shuffling  
D. One-hot encoding

28.  Which of the following is an example of a data formatting issue that can affect validation?

A. Outliers in numerical data  
B. **Inconsistent date formats (e.g., DD/MM/YYYY vs. MM/DD/YYYY)  
**C. Null values  
D. Large dataset size

29.  Discuss the problems to be considered while data integration

30.  List the techniques to identify redundant data

31.  List the strategies for data transformation

32.  Explain the following with example

a.      min-max normalization

b.     z-score normalization

c.      normalization by decimal scaling

33.  explain the concept of data generalization through concept hierarchy?

34.  Define data reduction

35.  List data reduction strategies

36.  What are the dimensionality reduction techniques

37.  Briefly discuss PCA?

38.  What data compression

39.  What are Parametric methods and non parametric methods?

40.  Define clustering

41.  What data sampling

42.  What is data cube aggregation

43.   Why is data integration important in data preprocessing?

A. It deletes unrelated records  
B. It converts data into images  
C. **It helps combine data from multiple sources for unified analysis**  
D. It creates duplicate copies of data

44.  Which of the following is an example of a schema mismatch problem?

A. Missing rows in a dataset  
B. **Different column names for the same attribute in two datasets**  
C. Duplicate records in a single file  
D. Extra white spaces in text fields

45.  What is the purpose of normalization in data transformation?

A. To reduce data size  
B. To convert data into images  
C. **To scale data values into a specific range (e.g., 0 to 1)**  
D. To create noise in data

46.  . In Pandas, which function is commonly used to merge two datasets based on a common column?

A. pd.append()  
B. pd.concat()  
C. pd.merge()  
D. pd.group()

47.  What are the Challenges & Applications of Data Reduction

48.  Which of the following is a common data reduction technique in data preprocessing?

A. Data Augmentation  
B. Data Cleaning  
**C. Principal Component Analysis (PCA)**  
D. Data Shuffling

49.  The main goal of data reduction is to:

A. Remove all noisy data from the dataset  
B. Increase the number of variables  
C. **Represent the data with less volume but produce similar analytical results**  
D. Normalize the data

50.  Explain the concept of dimensionality reduction and how PCA helps in reducing the complexity of large datasets.

51.  Discuss the benefits and challenges of using data cube aggregation for summarizing large datasets.

52.  How does attribute selection contribute to improving the efficiency of machine learning models, and what methods can be used for this process?

53.  Describe different data sampling techniques and their role in ensuring effective analysis while managing large volumes of data.

54.   What is Data Discretization

55.   Explain Unsupervised Discretization Methods

56.   Explain Clustering-Based Discretization

57.   Explain Entropy-Based Discretization

58.   Advantages & Limitations of Entropy-Based

59.   Which of the following statements best describes equal-frequency binning?

a.      It divides the range of values into intervals of equal width.

b.      It assigns each data point to a randomly selected bin.

**c.**      **It ensures each bin has approximately the same number of data points**

d.      It uses class labels to decide where to split the data.

60.   Which method of discretization uses Information Gain to determine the best split point for continuous attributes?

A) Equal-width binning  
B) K-means clustering  
C) **Entropy-based discretization**  
D) Z-score normalization

61.   Explain the difference between supervised and unsupervised discretization methods. Provide one example of each and describe how they work.

62.   Describe the steps involved in applying clustering-based discretization using the K-Means algorithm. What are the advantages and limitations of this method?

63.   What is entropy-based discretization, and how is Information Gain used to determine split points in continuous data? Provide a brief example to illustrate the process.

64.   Compare and contrast equal-width and equal-frequency binning in terms of bin distribution, sensitivity to outliers, and interpretability. When would you choose one over the other?

65.   Which method correctly reads data from a CSV file named "sales.csv"?

a) pd.open_csv("sales.csv")

b) pd.read_csv("sales.csv")

c) pd.load_csv("sales.csv")

d) pd.csv_reader("sales.csv")

66.   How would you select the "Name" and "Salary" columns from a DataFrame df?

a.      a) df("Name", "Salary")

b.      b) df[["Name", "Salary"]]

c.      c) df.select("Name", "Salary")

d.      d) df.columns("Name", "Salary")

67.   how missing values are represented

68.   what is the method for finding missing value?

69.   What is the method for removing missing value rows?

70.   Which of the following best describes the process of extracting useful patterns and knowledge from large datasets?

a.      Data Driven science

b.      Data Mining

c.      Big data

d.      Information retrival

71.   Data preprocessing, cleaning, and feature engineering

a.      **Data Scientist**

b.      b) **Machine Learning Engineer**  
c) **Data Analyst**

c.      d)None

72.   briefly discuss about descriptive statistics

73.   discuss the importance Exploratory Data Analysis

74.   what are the common EDA techniques

75.   what is data distribution and explain types of data destributions

76.   what are types of anomalies

77.   how to detect outliers

78.   explain skewed data distribution

79.   **Right-skewed distribution**

80.   **Left-skewed distribution**

81.   **Symmetrical distribution**

82.   **Write python code to Plot Histogram – Distribution of Marks**

83.   **Write python code to Plot Boxplot – Check for Outliers**

84.   **Write python code to Scatter Plot – Relationship between Marks and Attendance**

85.   **What are the data types in pandas?**

86.   **What is a series and dataframe in pandas?**

87.   **How to read the data from online**

88.   Explain the following attributes and it’s imporatance in pandas

a.      Dtypes

b.      Columns

c.      Index

89.   Differentiate describe() and info() in pandas

||**Make**|**Colour**|**Odometer (KM)**|**Doors**|**Price**|
|---|---|---|---|---|---|
|**0**|Toyota|White|150043|4|$4,000.00|
|**1**|Honda|Red|87899|4|$5,000.00|
|**2**|Toyota|Blue|32549|3|$7,000.00|
|**3**|BMW|Black|11179|5|$22,000.00|
|**4**|Nissan|White|213095|4|$3,500.00|
|**5**|Toyota|Green|99213|4|$4,500.00|
|**6**|Honda|Blue|45698|4|$7,500.00|
|**7**|Honda|Blue|54738|4|$7,000.00|
|**8**|Toyota|White|60000|4|$6,250.00|
|**9**|Nissan|White|31600|4|$9,700.00|

90.   Write python code Load above dataset into dataframe, calculate mean of “Odometer (KM)”  and sum of “Doors”

91.   Explain the difference between .loc & .iloc with example

||**Make**|**Colour**|**Odometer**|**Doors**|**Price**|
|---|---|---|---|---|---|
|**0**|Toyota|White|150043.0|4.0|$4,000|
|**1**|Honda|Red|87899.0|4.0|$5,000|
|**2**|Toyota|Blue|NaN|3.0|$7,000|
|**3**|BMW|Black|11179.0|5.0|$22,000|
|**4**|Nissan|White|213095.0|4.0|$3,500|
|**5**|Toyota|Green|NaN|4.0|$4,500|
|**6**|Honda|NaN|NaN|4.0|$7,500|
|**7**|Honda|Blue|NaN|4.0|NaN|
|**8**|Toyota|White|60000.0|NaN|NaN|
|**9**|NaN|White|31600.0|4.0|$9,700|

92.   Find the missing values of each column take appropriate action includes fill with mean or mode or median or constant or drop the rows analyse the scenario

93.   Add a new column to the above  dataset with constant value 10

94.    Group the data based on “make” and perform sum on Doors

95.   Filter the rows where “make == Honda” and display