

CO1:

1.      Define data science or what is data science

2.      Data science is combination of -------------

Statistics

Machine learning

Visualization

Data management

Mathematical optimization

Social science

Law


---


3.      List few data science application

Fraud detection

Recommender systems

Healthcare

Education

Transportation

Agriculture

Finance

E-Business

Retail


---

4.      **Briefly discuss the roles in data science**
5.      **What are the roles in data science**
### 1. Data Scientist

**Responsibilities:**

- Analyze and interpret complex data sets to extract insights
    
- Build predictive models and machine learning algorithms
    
- Preprocess, clean, and engineer features
    
- Communicate findings to non-technical stakeholders
    

### 2. Machine Learning Engineer

**Responsibilities:**

- Develop and deploy machine learning models
    
- Optimize models for performance and scalability
    
- Collaborate with data scientists to operationalize work
    
- Manage and maintain model infrastructure
    

### 3. Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
    
- Create visualizations and reports for decision-making
    
- Identify trends and patterns in data
    
- Collaborate with business stakeholders to understand data needs
    

### 4. Data Engineer

**Responsibilities:**

- Build and maintain data pipelines and ETL processes
    
- Manage data infrastructure and databases
    
- Ensure data quality, reliability, and availability
    
- Support data scientists and analysts with clean, accessible data
    

### 5. Business Intelligence (BI) Analyst

**Responsibilities:**

- Create dashboards and reports for business performance tracking
    
- Design data visualization tools for end-users
    
- Identify key performance indicators (KPIs) and metrics
    
- Collaborate with business teams to support decision-making
    

### 6. Data Architect

**Responsibilities:**

- Design and maintain data architectures
    
- Define data storage, integration, and processing strategies
    
- Ensure data security and compliance
    
- Collaborate with data engineers to implement data solutions
    

### 7. Statistician

**Responsibilities:**

- Apply statistical techniques to analyze data
    
- Conduct hypothesis testing and experiments
    
- Design surveys and experiments to gather data
    
- Provide statistical insights to support decision-making
    

### 8. AI/ML Researcher

**Responsibilities:**

- Conduct research to advance machine learning and AI
    
- Develop novel algorithms and models
    
- Publish research papers and contribute to conferences
    
- Collaborate with data scientists and engineers on cutting-edge solutions
    

### 9. Quantitative Analyst (Quant)

**Responsibilities:**

- Apply quantitative and mathematical methods to financial data
    
- Develop trading strategies and risk models
    
- Analyze market data to inform investment decisions
    
- Implement algorithms for trading and risk management
    

### 10. Chief Data Officer (CDO)

**Responsibilities:**

- Set organizational data strategy and governance
    
- Oversee data management, privacy, and compliance
    
- Align data initiatives with business goals
    
- Manage data-related teams and resources

---

6.      Tools required for data science

7.      Data science tools

- **Programming Languages:** Python, R, SQL, Julia.
- **Data Handling & Analysis:** Pandas, NumPy, Spark, Hadoop.
- **Visualization:** Matplotlib, Seaborn, Tableau, Power BI, Plotly.
- **Machine Learning & AI:** Scikit-learn, TensorFlow, PyTorch, XGBoost, Keras.
- **Big Data & Cloud:** Hadoop, Spark, AWS, Azure, Google Cloud.
- **Databases:** MySQL, PostgreSQL, MongoDB, Cassandra.
- **Collaboration & Versioning:** Git, Jupyter Notebooks, VS Code.
- **ETL & Data Pipelines:** Apache Airflow, Talend, Luigi.

| Toolbox      | Purpose                        | Example Use Case                         |
| ------------ | ------------------------------ | ---------------------------------------- |
| NumPy        | Numerical computing            | Fast array operations, linear algebra    |
| Pandas       | Data manipulation and analysis | DataFrames for tabular data              |
| Matplotlib   | Data visualization (2D plots)  | Line graphs, bar charts, scatter plots   |
| Seaborn      | Statistical data visualization | Boxplots, correlation graphs             |
| Scikit-learn | Machine learning               | Classification, regression, clustering   |
| TensorFlow   | Deep learning                  | Neural networks and deep learning models |
| Statsmodels  | Statistical modeling           | Regression, hypothesis testing           |
| Plotly       | Interactive visualization      | Dashboards, real-time plots              |

9.      Discuss few data science sample case studies

**Fraud Detection**

- **Goal:** Identify fraudulent activity **before it causes major damage**.
- **Approach:** Analyze historical data to detect patterns and anomalies.
- **Challenges:** Real-time detection is **harder than post-fact**, requires **high precision**.
- **Trade-offs:** Both false positives (innocent flagged) and false negatives (fraud missed) are costly.
- **Tech:** Stream processing, anomaly detection models, real-time dashboards, ML algorithms.

---

**Recommender Systems:**

- **Purpose:** Deliver **personalized suggestions** to users.
- **Impact:** Boosts sales, click-throughs, conversions.
- **Examples:**
    - **Netflix:** ~$1B/year value from recommendations.
    - **Amazon:** 20–35% annual sales lift.
- **Technique:** Collaborative filtering at scale, content-based methods, hybrid approaches.

---

**Patient Readmission Prediction:**

- **Goal:** Identify why patients return to the hospital.
- **Benefits:** Cut costs, improve population health.
- **Focus:** Understand **underlying causes** for specific populations.
- **Data:** Integrate multiple sources—EHRs, socioeconomic info, genetics, patient history.
- **Approach:** Analyze correlations between readmissions and health/social factors to enable targeted interventions.

---

**Smart Cities:**

- **Definition:** Use data + ICT to optimize urban living.
- **Goals:**
    - Plan communities efficiently
    - Manage infrastructure/assets effectively
    - Reduce operational costs
    - Use open data to engage citizens
---

10.      Who is the responsible for designing dashboards

###  Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
- Create visualizations and reports for decision-making
- Identify trends and patterns in data
- Collaborate with business stakeholders to understand data needs

### Business Intelligence (BI) Analyst

**Responsibilities:**

- Create dashboards and reports for business performance tracking
- Design data visualization tools for end-users
- Identify key performance indicators (KPIs) and metrics
- Collaborate with business teams to support decision-making

---

**11.What is the primary goal of data science?**
answer c
a.      To build websites

b.      To create artistic visualizations

c.      To extract insights from dataInformation retrival

d.      To maintain servers

---


**12.What is the correct sequence of a typical Data Science lifecycle?**

answer b

a.      Data Cleaning → Data Collection → Modeling → Deployment

b.      Data Collection → Data Cleaning → Modeling → Deployment

c.      Deployment → Modeling → Data Collection → Data Cleaning

d.      Modeling → Data Collection → Deployment → Data Cleaning

---

**13.Discuss the responsibilities of Data analyst , Data Scientist and Data Engineer**

### Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
- Create visualizations and reports for decision-making
- Identify trends and patterns in data
- Collaborate with business stakeholders to understand data needs

### Data Engineer

**Responsibilities:**

- Build and maintain data pipelines and ETL processes
- Manage data infrastructure and databases
- Ensure data quality, reliability, and availability
- Support data scientists and analysts with clean, accessible data

### Data Scientist

**Responsibilities:**

- Analyze and interpret complex data sets to extract insights
- Build predictive models and machine learning algorithms
- Preprocess, clean, and engineer features
- Communicate findings to non-technical stakeholders

---

14.      Explain the typical life cycle of data science project

- **Problem Definition:** Clarify business/analytical objective.
- **Data Collection:** Gather data from internal and external sources.
- **Data Cleaning & Preprocessing:** Handle missing values, outliers, formatting, and normalization.
- **Exploratory Data Analysis (EDA):** Understand distributions, correlations, patterns, and anomalies.
- **Feature Engineering:** Create, select, or transform features to improve model performance.
- **Modeling:** Apply statistical or ML algorithms to build predictive or descriptive models.
- **Evaluation:** Assess model performance using metrics, cross-validation, and testing.
- **Deployment:** Integrate model into production or decision-making systems.
- **Monitoring & Maintenance:** Track performance, update models with new data, ensure reliability.

---

**15.Explain the following python libraries with example use case**

Numpy

Pandas

Scipy

Sikit-learn

Matplotlib


**NumPy** – Core numerical library for arrays and fast math.  
*Use case*: Vectorized matrix multiplication for ML.  
```python
import numpy as np
A = np.array([[1,2],[3,4]])
B = np.array([[5,6],[7,8]])
print(A @ B)  # matrix product
```


**Pandas** – Data analysis library with DataFrame for tabular data.  
*Use case*: Load CSV and filter rows.  

```python
import pandas as pd
df = pd.DataFrame({"Name":["A","B"],"Marks":[80,90]})
print(df[df["Marks"]>85])
```

**SciPy** – Scientific computing (stats, optimization, signal).  
*Use case*: Statistical test.  
```python
from scipy import stats
a = [1,2,3,4,5]
b = [2,3,4,5,6]
print(stats.ttest_ind(a, b))
```


**Scikit-learn** – ML toolkit (classification, regression, clustering).  
*Use case*: Train a classifier.  
```python
from sklearn.linear_model import LogisticRegression
X = [[0],[1],[2],[3]]
y = [0,0,1,1]
model = LogisticRegression().fit(X,y)
print(model.predict([[1.5]]))
```

**Matplotlib** – Plotting and visualization.  
*Use case*: Line chart.  
```python
import matplotlib.pyplot as plt
plt.plot([1,2,3],[2,4,6])
plt.xlabel("X")
plt.ylabel("Y")
plt.show()
```

**18. Integrate a function of one variable (0 to 1)**

```python
from scipy import integrate 
f = lambda x: 3*x**2 + 1 
result, _ = integrate.quad(f, 0, 1)
print("Definite integral:", result)
  ```

---

**19. Integrate a function of two variables (0 to 1 for both)**

```python
f = lambda x, y: 3*x**2 + 1 
result, _ = integrate.dblquad(f, 0, 1, lambda x: 0, lambda x: 1) 
print("Double integral:", result)
```

---

**20/21. Supervised vs Unsupervised Learning**

- **Supervised Learning** – Labeled data, model learns input → output mapping.  
    _Example (Scikit-learn)_:
    

```python 
from sklearn.linear_model import LinearRegression 
X = [[1],[2],[3]] 
y = [2,4,6] model = LinearRegression().fit(X,y) print(model.predict([[4]]))```

- **Unsupervised Learning** – No labels, model finds patterns/clusters.  
    _Example (Scikit-learn)_:
    

```python
from sklearn.cluster import KMeans
X = [[1],[2],[10],[12]]
kmeans = KMeans(n_clusters=2).fit(X) 
print(kmeans.labels_)
```

---

**22. Steps in building an ML model (Scikit-learn)**

1. Data collection
    
2. Data preprocessing (cleaning, scaling, encoding)
    
3. Split dataset (train/test)
    
4. Choose algorithm
    
5. Train model (`fit`)
    
6. Evaluate model (`predict` + metrics)
    
7. Tune hyperparameters
    
8. Deploy / use for predictions
    

---

**23. Classification model evaluation metrics**

- Accuracy, Precision, Recall, F1-score
    
- Confusion matrix
    
- ROC-AUC curve
    
- Log loss
    

---

**24. Define Data Analytics**  
Extraction, transformation, and analysis of data to generate insights and support decision-making.

---

**25. Pip and Conda**

- **Pip** – Python’s standard package installer (`pip install package`).
    
- **Conda** – Environment and package manager (Python + non-Python packages) for reproducibility.
    

---

**26. Python package management tools**

- **Pip** – Installs packages from PyPI.
    
- **Conda** – Manages environments and packages; works for Python + other binaries.
    
- **Miniconda** – Lightweight Conda installer; minimal base setup for creating environments.


**27.How many ways you can launch jupyter notebook**

- Terminal / Command Prompt: `jupyter notebook`
    
- Anaconda Navigator → Launch Jupyter Notebook
    
- VS Code → Open Jupyter Notebook

**28.Write a command to install new python libraries on terminal or anaconda power shell**

- Using pip:
    

`pip install package_name`

- Using conda:
    

`conda install package_name`

**29.Write a steps to create your own virtual environment(venv)**

1. Open terminal / CMD
    
2. Create env:
    

`python -m venv myenv`

3. Activate env:
    

- Windows: `myenv\Scripts\activate` 

4. Install packages within this environment
    
5. Deactivate when done: `deactivate`


**30.How to update a package using conda**

conda update package_name

**31.Write a code to display the version of package**


- Using pip:
    

`pip show package_name`

- Using Python:
    

`import package_name print(package_name.__version__)`

- Using conda:
    

`conda list package_name`


---


**32.What are types of data**

a.      Record Data

b.      Transaction Data

c.      Graph Data (Network Data)

d.      Spatial Data

e.      Time-Series Data

f.       Text Data

g.      Sequence Data

h.      Hierarchical Data

### Record Data

A collection of records, each with the same set of attributes.

- **Examples:** Relational databases, Excel spreadsheets
- **Format:** Rows = objects, Columns = attributes

**Example Table:**

| Name  | Age | Dept |
| ----- | --- | ---- |
| Alice | 20  | CSE  |
| Bob   | 22  | ECE  |

### Transaction Data

A set of transactions where each transaction is a set of items.

- **Examples:** Market basket data, Online purchase logs

**Example Table:**

| TID | Items               |
| --- | ------------------- |
| 1   | Milk, Bread, Butter |
| 2   | Bread, Eggs         |
| 3   | Milk, Eggs, Diaper  |

### Graph Data (Network Data)

Consists of **nodes** (objects) and **edges** (relationships).

- **Used for:** Social networks, web analysis, communication networks
- **Example (Social Network):**
    - Nodes: People (e.g., Alice, Bob, Carol)
    - Edges: Friendships (e.g., Alice ↔ Bob, Bob ↔ Carol)

### Spatial Data

Objects have **spatial locations** (coordinates).

- **Use Case:** Google Maps, satellite imagery, urban planning
- **Used for:** Maps, GPS, GIS (Geographic Information Systems)
- **Example:**
    - A point: `(latitude, longitude)` → `(17.385, 78.4867)`
    - A region: polygon boundaries of a city

#### Categorical Data

**Definition:** Data that represents categories or groups.  
**Characteristics:**

- No inherent numeric meaning
- Arithmetic operations are not meaningful  
    **Examples:**
- Gender (Male, Female)
- Color (Red, Blue, Green)
- Department (HR, Sales, IT)

#### Numerical Data

**Definition:** Data that consists of numbers and can be measured.  
**Types:**

- **Discrete:** Countable values (e.g., number of children)
- **Continuous:** Measurable quantities (e.g., height, weight)  
    **Examples:**
- Age = 25
- Salary = $50,000

#### Ordinal Data

**Definition:** Categorical data with a clear ordering or ranking.  
**Characteristics:**

- Order matters, but the difference between values is not meaningful  
    **Examples:**
- T-shirt size (Small < Medium < Large)
- Customer satisfaction (Poor < Fair < Good < Excellent)

#### Binary Data

**Definition:** Data with only two possible values.  
**Types:**

- **Symmetric:** Both outcomes are equally important (e.g., Male/Female)
- **Asymmetric:** One outcome is more important (e.g., Disease: Yes/No)  
    **Examples:**
- Yes/No
- True/False

---

**33.What is nominal type attribute and explain with example**

#### Nominal Attributes

**Definition:** Names or labels with no ordering  
**Operations:** Equality comparison  
**Examples:** Colors, ZIP codes

---
**34.What is ordinal type attribute and explain with example**

#### Ordinal Attributes

**Definition:** Ordered categories  
**Operations:** Comparisons such as `<`, `>`  
**Examples:** Rankings, grades

---
**35.Differentiate Discrete and continuous attributes with examples**

|Attribute Type|Definition|Example|
|---|---|---|
|**Discrete**|Takes countable, separate values|Number of students in a class: 20, 25, 30|
|**Continuous**|Can take any value in a range (measurable)|Height of students: 150.5 cm, 162.3 cm, 170.0 cm|

---

**36.List qualitative attributes**

- **Nominal** (no natural order):
    
    - Color (Red, Blue, Green)
        
    - Gender (Male, Female, Other)
        
    - Blood Group (A, B, AB, O)
        
    - Nationality (Indian, American, etc.)
        
    - Product Type (Electronics, Furniture)
        
- **Ordinal** (has natural order):
    
    - Education Level (High School < Bachelor < Master < PhD)
        
    - Customer Satisfaction (Poor < Average < Good < Excellent)
        
    - Military Rank (Private < Sergeant < Captain < General)
        

---

**37.List quantitative attributes**

- **Discrete** (countable values):
    
    - Number of siblings
        
    - Number of students in a class
        
    - Number of cars owned
        
- **Continuous** (measurable values, can take any value in a range):
    
    - Height (cm)
        
    - Weight (kg)
        
    - Salary (₹)
        
    - Temperature (°C)

---

**38.Researcher wants to analyze the relationship between student performance and hours studied. What type of dataset would they likely use? What kind of analysis techniques would be suitable?**

**Dataset Type:** Quantitative (numerical) dataset with at least two variables:

- Independent variable: Hours studied (continuous)
    
- Dependent variable: Student performance/marks (continuous)
    

**Analysis Techniques:**

- **Correlation Analysis:** To measure strength and direction of relationship.
    
- **Regression Analysis:**
    
    - Simple Linear Regression: Predict performance from hours studied.
        
    - Multiple Regression (if including more factors like attendance, sleep).
        
- **Visualization:** Scatter plots to observe trends and patterns.

**39.Discuss how to Measure Data Similarity and Dissimilarity**
#### Similarity

Numerical measure of how alike two data objects are.

- Value is higher when objects are more alike.
- Often falls in the range ([0, 1]).

**What it is:** A number that tells us how much two data points are alike.

- Higher value → more alike.
- Usually ranges from 0 (not alike) to 1 (exactly alike).

#### Dissimilarity (e.g., distance)

Numerical measure of how different two data objects are.

- Lower when objects are more alike.
- Minimum dissimilarity is often 0.
- Upper limit varies.

**What it is:** A number that tells us how different two data points are.

- Lower value → more alike.
- Minimum = 0 (identical points).
- Maximum can vary.

**Proximity** refers to a similarity or dissimilarity. **Proximity** is just a general term for either similarity or dissimilarity.


they can be calculated by using 

- Euclidean distance formula:

$$
d(x_i, x_j) = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \dots + (x_{ip}-x_{jp})^2}
$$

- $x_i, x_j$ = objects  
- $p$ = number of attributes  
- Dissimilarity matrix is **symmetric**: $d(x_i, x_j) = d(x_j, x_i)$  
- Useful for clustering or similarity analysis

**40.What is symmetric and asymmetric binary variables**

#####  Symmetric vs Asymmetric Binary Attributes

|Attribute Type|Definition|Count 0?|Example|
|---|---|---|---|
|Symmetric|Both outcomes are equally meaningful|Yes|Gender (M/F), Yes/No survey response|
|Asymmetric|Only one outcome is meaningful; 0 carries little/no info|No (ignore 0-0 matches)|Fever presence, Positive test result|

**Rule of Thumb:**

- Ask: “Does absence (0) carry information?”
    - Yes → Symmetric
    - No → Asymmetric

---

**41.What is the distance measure for symmetric binary variable**

#### Distance Measure for Symmetric Binary Variables

For symmetric binary variables, both 1s and 0s matter equally. The distance between objects i and j is:

Explanation:

- **Numerator (r + s):** Number of mismatches between i and j
- **Denominator (q + r + s + t):** Total number of attributes

> Higher distance → more dissimilar objects.  
> Lower distance → more similar objects.

Where:

- **q** = number of attributes where both i and j are 1
- **r** = number of attributes where i = 1, j = 0
- **s** = number of attributes where i = 0, j = 1
- **t** = number of attributes where both are 0
- **p** = total number of attributes

**42.What is the distance measure for asymmetric binary variable**

#### Jaccard Coefficient (Similarity Measure for Asymmetric Binary Variables)

The **Jaccard coefficient** measures similarity rather than distance:

Explanation:

- **Numerator (q):** Number of attributes where both i and j are 1
- **Denominator (q + r + s):** Attributes where at least one object has 1

> Value ranges from 0 (completely different) to 1 (identical).

Where:

- **q** = number of attributes where both i and j are 1
- **r** = number of attributes where i = 1, j = 0
- **s** = number of attributes where i = 0, j = 1
- **t** = number of attributes where both are 0
- **p** = total number of attributes


Calculate the distance between asymmetric binary variables between (jack,mary) and (jack,jim) and what is your inference with the output

---

**43.Calculate the distance between asymmetric binary variables between (jack,mary) and (jack,jim) and what is your inference with the output**

| Name | Gender | Fever | Cough | Test-1 | Test-2 | Test-3 | Test-4 |
| ---- | ------ | ----- | ----- | ------ | ------ | ------ | ------ |
| Jack | M      | Y     | N     | P      | N      | N      | N      |
| Mary | F      | Y     | N     | P      | N      | P      | N      |
| Jim  | M      | Y     | P     | N      | N      | N      | N      |

44.**Calculate the similarity distance between given points using Euclidean, Manhattan, and Supremum distance measures and draw conclusions.**

![[Pasted image 20250829112414.png]]


---

**45.What is cosine similarity and explain with case study**

#### Cosine Similarity

If **d1** and **d2** are two vectors (e.g., term-frequency vectors), then:
$$
cos(d1,d2)=d1⋅d2∥d1∥∥d2∥\text{cos}(d_1, d_2) = \frac{d_1 \cdot d_2}{\|d_1\| \|d_2\|}
$$

Where:

- *⋅* indicates the vector dot product
    
- ∥d∥ is the length (magnitude) of vector **d**
    

####  Example

##### Term-Frequency Table

|Document|teamcoach|hockey|baseball|soccer|penalty|score|win|loss|season|
|---|---|---|---|---|---|---|---|---|---|
|Document|5|0|3|0|2|0|0|2|0|
|Document2|3|0|2|0|1|1|0|1|0|

##### Cosine Similarity Calculation Example

###### Vectors
$$
d1=[5,0,3,0,2,0,0,2,0],d2=[3,0,2,0,1,1,0,1,0]
$$
###### Dot Product
$$
d1⋅d2=5∗3+0∗0+3∗2+0∗0+2∗1+0∗1+0∗0+2∗1+0∗0=25 
$$
###### Magnitudes

$$
\|d_1\| = \sqrt{5^2 + 0^2 + 3^2 + 0^2 + 2^2 + 0^2 + 0^2 + 2^2 + 0^2} = \sqrt{42} \approx 6.48 $$

$$\|d_2\| = \sqrt{3^2 + 0^2 + 2^2 + 0^2 + 1^2 + 1^2 + 0^2 + 1^2 + 0^2} = \sqrt{16} = 4
$$
###### Cosine Similarity
$$
\cos(d_1, d_2) = \frac{25}{6.48 * 4} \approx 0.964
$$
###### Interpretation

The two documents are highly similar (~0.96 cosine similarity).

---

**46.Distinguish between symmetric and asymmetric binary variables when measuring similarity.**

#####  Symmetric vs Asymmetric Binary Attributes

|Attribute Type|Definition|Count 0?|Example|
|---|---|---|---|
|Symmetric|Both outcomes are equally meaningful|Yes|Gender (M/F), Yes/No survey response|
|Asymmetric|Only one outcome is meaningful; 0 carries little/no info|No (ignore 0-0 matches)|Fever presence, Positive test result|

**Rule of Thumb:**

- Ask: “Does absence (0) carry information?”
    - Yes → Symmetric
    - No → Asymmetric
---

**49.What is the function to  list of all the variable and function names in the module.**

answer dir

      Dir()

      Head()

      Tail()

      Describe()
---      
 **50.Write a python code for displaying single dimensional array using for loop**

```python
import numpy as np

# Create a 1D array
arr = np.array([10, 20, 30, 40, 50])

# Display elements using for loop
for element in arr:
    print(element)
```
---
51.Write a python code for sorting given array [65,34,67,10,45,2]

```python
arr = [65, 34, 67, 10, 45, 2]

# Sort the array
arr.sort()

# Display sorted array
print(arr)
```
---
**52.Declare an array with elements 1..10 and write code for displaying even numbers**

```python
import numpy as np

# Declare array with elements 1 to 10
arr = np.arange(1, 11)

# Display even numbers
for num in arr:
    if num % 2 == 0:
        print(num)
```
---
**53.Declare an array with 9 elements and write a code to display elements from 3 to 7**
```python
import numpy as np

# Declare array with 9 elements
arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])

# Display elements from index 2 to 6 (3rd to 7th element)
for num in arr[2:7]:
    print(num)
```

---
**54.How to check the array is empty or not**
```python
import numpy as np

# Example arrays
arr1 = np.array([])
arr2 = np.array([1, 2, 3])

# Check if empty
print(arr1.size == 0)  # True
print(arr2.size == 0)  # False
```

---
**55.What is ndim attribute**

```python
import numpy as np

arr = np.array([[1, 2, 3], [4, 5, 6]])
print(arr.ndim)  # Output: 2
```
---

**56.What is shape and reshape of an array with examples**

```python
import numpy as np

# Original array
arr = np.array([1, 2, 3, 4, 5, 6])
print(arr.shape)  # Output: (6,)

# Reshape to 2 rows and 3 columns
arr2 = arr.reshape(2, 3)
print(arr2)
print(arr2.shape)  # Output: (2, 3)
```

---

**57.Write a code to create 5X5 identity matrix**
```python
import numpy as np

# Create 5x5 identity matrix
identity_matrix = np.eye(5)

print(identity_matrix)
```

**58.What is linespace() in numpy**

`linspace()` in NumPy generates **evenly spaced numbers over a specified interval**.

Syntax:

`numpy.linspace(start, stop, num=50, endpoint=True)`

- `start` – starting value
    
- `stop` – ending value
    
- `num` – number of values to generate (default 50)
    
- `endpoint` – if True, includes `stop`
    

Example:

`import numpy as np  arr = np.linspace(0, 10, 5) print(arr)`

---

**59.Write a code to create diagonal matrix**

```python
import numpy as np

# Elements for the diagonal
diagonal_elements = [1, 2, 3, 4, 5]

# Create diagonal matrix
diag_matrix = np.diag(diagonal_elements)

print(diag_matrix)

```
---
  **60.What is the method for calculating sum of diagonal elements in a matrix**

```python
import numpy as np

matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

diag_sum = np.trace(matrix)
print(diag_sum)  # Output: 15
```
---
**61.Discuss the possibilities to transpose a matrix using numpy**

In NumPy, a matrix can be transposed in multiple ways:

1. **`.T` attribute** – simplest:
    

```python
import numpy as np
mat = np.array([[1, 2], [3, 4]])
print(mat.T)
```

2. **`np.transpose()` function** – general, can reorder axes for higher dimensions:
    

```python
print(np.transpose(mat))
```

3. **`swapaxes()`** – swap two specified axes (useful in >2D arrays):
    

```python
print(mat.swapaxes(0, 1))
```

All produce the same result for 2D arrays:

```
[[1 3]
 [2 4]]
```
---
**62.Discuss the accessing elements with indexes

1. **1D array** – single index:
    

```python
import numpy as np arr = np.array([10, 20, 30, 40]) print(arr[2])  # Output: 30
```

2. **2D array** – row and column indices:
    

```python
mat = np.array([[1, 2], [3, 4]]) print(mat[1, 0])  # Output: 3 (row 1, column 0)
```

3. **Negative indexing** – count from end:
    

```python
print(arr[-1])  # Output: 40
```

4. **Slicing** – access ranges:
    

```python
print(arr[1:3])  # Output: [20 30] 
print(mat[:, 1])  # Output: [2 4] (all rows, column 1)
```

Indexes can be **single integers, slices, or lists/arrays** for advanced selection.

---
---
---
CO2:

1.      Define structured data and give one example.

 Structured Data

**Definition**: Structured data is highly organized, typically in tabular formats with predefined schemas (rows and columns). It’s easily searchable and ideal for relational databases.

**Characteristics**:

- Fixed fields: Each data point fits a specific column (e.g., Name, Age).
    
- Consistent format: Enables efficient querying using structured query languages.
    
- Examples: Customer databases, financial records, inventory logs.
    

**Storage**:

- **SQL Databases**: MySQL, PostgreSQL, Oracle for relational data.
    
- **Files**: CSV, Excel, or parquet for portable tabular data.
    

**Examples**:

- **Customer Information**: A table storing names, ages, and emails.
    
- **Financial Data**: Sales transactions with columns for date, amount, and product ID.
    

**Sample Customer Table**:


| Name  | Age | Email           |
|-------|-----|-----------------|
| John  | 28  | john@email.com  |
| Sarah | 34  | sarah@email.com |
| Mike  | 25  | mike@email.com  |

---

2.      What is the role of data cleaning in data preprocessing?
Data cleaning is a foundational step in the data preprocessing pipeline, addressing issues in raw data to ensure it’s suitable for analysis, visualization, or modeling. 

## Definition and Goal

- **Process**: Detecting and correcting **incomplete, inaccurate, inconsistent, or irrelevant data**.
- **Techniques**: Modify or remove corrupt/unusable records.
- **Goal**: Ensure high-quality data before further processing.



## Importance

- "Data cleaning is one of the three biggest problems in data processing." — _Ralph Kimball_
- "Data cleaning is the number one problem in data processing." — _DCI Survey_



## Typical Tasks

- Fill in **missing values** (e.g., mean, median, interpolation). Replace or remove gaps to ensure completeness. For example, in a dataset with missing salaries, imputing the mean ($mean = \frac{\sum x_i}{n}$) maintains statistical properties.

- Identify **outliers** and smooth noisy data (e.g., binning, regression). Outliers (e.g., a temperature reading of 100°C in a room) distort analyses; smoothing via binning or regression corrects them.

- Correct **inconsistent data** (e.g., conflicting codes, formats).  Standardizing formats (e.g., dates as YYYY-MM-DD) prevents mismatches in joins or time-series analysis.

- Resolve **redundancy** from data integration (e.g., duplicates).  Duplicates from merging datasets (e.g., same customer in two CRM systems) inflate counts, requiring deduplication.

Each task addresses a specific data quality dimension (completeness, accuracy, consistency), ensuring the dataset is ready for tasks like regression ($Y = \beta_0 + \beta_1 X$) or visualization (e.g., accurate bar charts).

---
3.      Mention any two data collection strategies in the context of data science and visualization.


 Effective data collection ensures quality insights. It’s an iterative process requiring clear objectives, source identification, and ethical considerations.

1. **Define Clear Objectives**
    
    - Specify goals: “Understand customer retention drivers.”
        
    - Questions: “What factors reduce churn?”
        
    - Example: A telecom company defines KPIs like “churn rate” to collect call logs and feedback.
        
2. **Identify Relevant Data Sources**
    
    - Sources: Internal (databases), external (APIs, public datasets).
        
    - KPIs: “Customer lifetime value,” “website visits.”
        
    - Example: Use Google Analytics API to fetch click-through rates (ga:pageviews).
        
3. **Data Quality Assessment**
    
    - Evaluate: Completeness (no missing values), accuracy (correct values), consistency (no conflicts).
        
    - Clean: Remove duplicates, fix formats.
        
    - Example: In a dataset, flag rows with missing emails using df[df['Email'].isnull()].
        
4. **Consider Structured and Unstructured Data**
    
    - Combine: Sales data (structured) with reviews (unstructured).
        
    - Example: Merge CRM sales with Twitter sentiment for a holistic customer view.
        
5. **Real-Time Data Collection**
    
    - Use streaming: Kafka for live stock prices.
        
    - Example: IoT sensors stream temperature every second for real-time factory monitoring.
        
6. **Data Privacy and Ethics**
    
    - Comply: GDPR requires consent for EU user data.
        
    - Example: Anonymize names (e.g., “John” → “User123”) before analysis.
        
7. **Sampling Techniques**
    
    - Subsets: Random sample 10% of 1M rows.
        
    - Example: Use df.sample(frac=0.1, random_state=42) to test models efficiently.
        
8. **Surveys and Questionnaires**
    
    - Targeted: Ask “Rate service (1-5)” for feedback.
        
    - Example: Survey 1000 customers, ensuring questions align with “satisfaction” KPIs.
        
9. **Collaboration with Stakeholders**
    
    - Engage experts: Sales team suggests “deal size” as a metric.
        
    - Example: Meet with marketing to prioritize data on campaign conversions.
        
10. **Data Integration**
    
    - Unify: Merge CRM and ERP data.
        
    - Example: Combine customer IDs across platforms, resolving duplicates with df.drop_duplicates('CustomerID').

---

4.      What is personally identifiable information (PII) and why is it important in data privacy?

**Personally Identifiable Information (PII)** is any data that can directly or indirectly identify an individual, such as name, email, phone number, Social Security number, or biometric data.

**Importance in data privacy:** Protecting PII prevents identity theft, fraud, and unauthorized profiling, ensuring legal compliance (e.g., GDPR, HIPAA) and maintaining trust between organizations and individuals.

----
5.      Name any two techniques used to ensure data integrity.

1. **Checksums/Hashing** – verifies that data hasn’t been altered during storage or transmission.
    
2. **Database Constraints** – rules like primary keys, foreign keys, and unique constraints enforce consistent and valid data.

---

6.      What is the main purpose of data masking?

The main purpose of **data masking** is to **protect sensitive information** by replacing it with realistic but fictitious values, allowing data to be used for testing or analysis without exposing actual PII or confidential data.

----

7.      Give one difference between structured and semi-structured data with examples.

- **Structured data** – organized in fixed rows and columns; easily stored in databases.  
    _Example:_ Employee table with columns: ID, Name, Salary.
    
- **Semi-structured data** – has some organizational tags but no fixed schema.  
    _Example:_ JSON or XML files storing user profiles.

---

8.      Explain the three types of data sources—structured, unstructured, and semi-structured—with examples, and discuss their implications in data preprocessing.

**Structured Data:**

- **Definition:** Data organized in a fixed schema, rows, and columns.
    
- **Example:** SQL tables, Excel sheets with employee records.
    
- **Preprocessing Implications:** Minimal cleaning needed; focus on handling missing values, normalization, and encoding categorical features.
    

**2. Unstructured Data:**

- **Definition:** Data with no predefined format or schema.
    
- **Example:** Emails, images, videos, social media posts, PDFs.
    
- **Preprocessing Implications:** Requires heavy preprocessing like text tokenization, image resizing, feature extraction, and noise removal.
    

**3. Semi-structured Data:**

- **Definition:** Data with organizational tags or markers but no strict schema.
    
- **Example:** JSON, XML, log files.
    
- **Preprocessing Implications:** Need parsing, schema inference, flattening nested structures, and converting to structured formats for analysis.

---

9.     Describe in detail the various data collection strategies used in data science and visualization, highlighting how each contributes to quality insights.

- **Surveys and Questionnaires**
    
    - **Method:** Collect responses directly from users via forms or interviews.
        
    - **Contribution:** Provides targeted, structured data; ensures relevance and context; high-quality insights if questions are precise.
        
- **Web Scraping & APIs**
    
    - **Method:** Extract data from websites or online services using scripts or API endpoints.
        
    - **Contribution:** Accesses large-scale, real-time data; enables trend analysis and dynamic visualization; requires cleaning to ensure accuracy.
        
- **Transactional and Log Data**
    
    - **Method:** Collect internal data from business transactions, user interactions, or system logs.
        
    - **Contribution:** Captures real behavioral patterns; supports predictive analytics; must be processed for consistency.
        
- **Sensors and IoT Devices**
    
    - **Method:** Automated collection from devices like temperature sensors, GPS trackers, or wearables.
        
    - **Contribution:** High-frequency, continuous data; supports time-series analysis and monitoring; requires noise filtering.
        
- **Public & Open Data Repositories**
    
    - **Method:** Use datasets from government portals, research institutions, or open-source databases.
        
    - **Contribution:** Cost-effective, large datasets for benchmarking or modeling; verify source credibility for quality.
        
- **Experiments and A/B Testing**
    
    - **Method:** Controlled experiments to test hypotheses.
        
    - **Contribution:** Provides causal insights; supports data-driven decision making; ensures high validity if design is sound.
        
- **Social Media & User-Generated Content**
    
    - **Method:** Collect posts, comments, and interactions from platforms.
        
    - **Contribution:** Captures sentiment, trends, and engagement; requires NLP and preprocessing to extract meaningful insights.

---

10.  Discuss the different data security issues mentioned in the session and explain methods to address each.

Security is critical to protect data integrity, privacy, and compliance during analysis and visualization.

1. **Data Breaches**
    
    - Risks: Financial loss, reputational damage.
        
    - Example: A hacker accesses credit card data, leading to $1M in fraudulent charges.
        
2. **Data Privacy**
    
    - Protect PII: Use anonymization (e.g., hash emails).
        
    - Differential Privacy: Add noise to aggregates. Example: Report average salary with noise to hide individual values.
        
3. **Data Access Control**
    
    - RBAC: Analysts read, admins write.
        
    - Example: Grant SELECT only to data scientists in SQL DBs.
        
4. **Data Encryption**
    
    - In transit: HTTPS for APIs. At rest: AES-256 for files.
        
    - Example: Encrypt customer data in S3 using aws s3 cp --sse AES256.
        
5. **Data Integrity**
    
    - Verify: Use SHA-256 checksums.
        
    - Example: Check file hash post-transfer with sha256sum data.csv.
        
6. **Secure Data Sharing**
    
    - Methods: SFTP, secure APIs.
        
    - Example: Share reports via expiring links in OneDrive.
        
7. **Data Masking and Redaction**
    
    - Mask: Replace PII (e.g., “1234-5678-9012-3456” → “XXXX-XXXX-XXXX-3456”).
        
    - Example: Redact names in shared datasets using regex in Python.
        
8. **Compliance with Regulations**
    
    - Laws: GDPR, HIPAA.
        
    - Example: Obtain explicit consent for collecting EU customer data.
        
9. **Awareness and Training**
    
    - Educate: Phishing workshops.
        
    - Example: Train staff to avoid sharing passwords.
        
10. **Data Lifecycle Management**
    
    - Secure: Storage to deletion.
        
    - Example: Schedule auto-deletion of logs after 1 year with cron jobs.


---

11.  Explain the complete process of data preprocessing, including cleaning, integration, transformation, reduction, and discretization, with examples.

**Data Preprocessing Process:** A crucial step in data science to ensure high-quality, usable data.

1. **Data Cleaning**
    
    - **Purpose:** Remove errors, inconsistencies, and missing values.
        
    - **Techniques & Examples:**
        
        - Handle missing values: `df.fillna(0)` or `df.dropna()`
            
        - Remove duplicates: `df.drop_duplicates()`
            
        - Correct inconsistent entries: e.g., unify "Male"/"M" → "Male"
            
2. **Data Integration**
    
    - **Purpose:** Combine data from multiple sources into a unified dataset.
        
    - **Techniques & Examples:**
        
        - Merge tables: `pd.merge(df1, df2, on='ID')`
            
        - Resolve conflicts in schema or units (e.g., currency conversion)
            
3. **Data Transformation**
    
    - **Purpose:** Convert data into a suitable format or scale for analysis.
        
    - **Techniques & Examples:**
        
        - Normalization/Standardization: scale features to 0–1 or z-scores
            
        - Encoding categorical variables: `pd.get_dummies()`
            
        - Log or Box-Cox transformations for skewed data
            
4. **Data Reduction**
    
    - **Purpose:** Reduce data volume while retaining essential information.
        
    - **Techniques & Examples:**
        
        - Dimensionality reduction: PCA (`sklearn.decomposition.PCA`)
            
        - Aggregation: sum sales per month instead of daily
            
        - Sampling: use a representative subset for large datasets
            
5. **Discretization**
    
    - **Purpose:** Convert continuous attributes into discrete intervals or bins.
        
    - **Techniques & Examples:**
        
        - Equal-width binning: `pd.cut(df['age'], bins=5)`
            
        - Equal-frequency binning: `pd.qcut(df['income'], q=4)`
            

**Summary Flow:**  
`Raw Data → Cleaning → Integration → Transformation → Reduction → Discretization → Ready for Analysis`

---

12.  What is data encryption? Explain the importance of encryption in transit and at rest with suitable examples.

**Data Encryption:** The process of converting data into an unreadable format (ciphertext) using algorithms, so only authorized parties with the decryption key can access it.

**Importance:**

1. **Encryption in Transit**
    
    - **Purpose:** Protects data while it moves across networks.
        
    - **Example:** HTTPS encrypts web traffic between browser and server to prevent eavesdropping or man-in-the-middle attacks.
        
2. **Encryption at Rest**
    
    - **Purpose:** Secures stored data against unauthorized access.
        
    - **Example:** Databases or cloud storage using AES encryption to protect sensitive customer information even if the storage device is compromised.

---

13.  Discuss the importance of data privacy in analytics and visualization and explain how anonymization and differential privacy help maintain it.

**Importance of Data Privacy:** Protecting personal or sensitive data in analytics and visualization prevents misuse, legal violations (GDPR, HIPAA), and loss of trust while enabling meaningful insights without exposing individuals.

**Techniques to Maintain Privacy:**

1. **Anonymization**
    
    - Removes or masks personally identifiable information (PII) so individuals cannot be linked to the data.
        
    - _Example:_ Replacing names and emails with random IDs in a dataset before analysis.
        
2. **Differential Privacy**
    
    - Adds controlled noise to datasets or query results to prevent identification of individuals while preserving overall patterns.
        
    - _Example:_ A survey platform reporting aggregate statistics with noise to protect respondent identities.
---

14.  Describe the concept of compliance with regulations in data handling and explain the consequences of non-compliance with examples such as GDPR and HIPAA.

**Compliance in Data Handling:** Adhering to laws, standards, and policies that govern collection, storage, processing, and sharing of data to protect privacy and security.

**Consequences of Non-Compliance:**

1. **Legal Penalties:** Fines, lawsuits, or criminal charges.
    
    - _Example:_ GDPR can impose fines up to €20 million or 4% of annual global turnover for breaches.
        
2. **Reputational Damage:** Loss of customer trust and market credibility.
    
    - _Example:_ Healthcare organizations violating HIPAA may face public backlash alongside fines.
        
3. **Operational Risks:** Forced audits, restrictions on data usage, or shutdown of services.
    

**Examples:**

- **GDPR (EU):** Protects EU citizens’ personal data; mandates consent, right to be forgotten, and breach reporting.
    
- **HIPAA (US):** Secures patient health information; violations can lead to fines and legal actions.
---

15.  Define noise,incomplete, inconsistent

**1. Noise:** Random errors or irrelevant data that distort true values.  
_Example:_ Sensor readings fluctuating due to interference.

**2. Incomplete Data:** Missing or partially recorded information.  
_Example:_ A customer record without an email or phone number.

**3. Inconsistent Data:** Conflicting or contradictory information within a dataset.  
_Example:_ Same customer listed with different birthdates in two tables.

---

16.  What are the measures to asses the data quality
### Core Dimensions
- **Accuracy** → Value correctly represents the real-world fact.  
  *Ex: Recorded temperature = 25°C, actual = 25°C.*  

- **Completeness** → All required data is present.  
  *Ex: Customer record missing phone number → incomplete.*  

- **Consistency** → No contradictions across datasets.  
  *Ex: `DOB = 2000-01-01` in one table, `DOB = 1999-12-31` in another → inconsistent.*  

- **Timeliness** → Data is up-to-date.  
  *Ex: Stock price updated hourly vs real-time feed.*  

- **Believability** → Data is credible and trustworthy.  
  *Ex: Sales data from official ERP vs. an unverified Excel sheet.*  

- **Value Added** → Data contributes to decision-making.  
  *Ex: Adding “Customer Lifetime Value” helps marketing strategy.*  

- **Interpretability** → Data is easy to understand.  
  *Ex: Column named `salary_in_usd` vs. `sal1`.*  

- **Accessibility** → Data is available when needed.  
  *Ex: Secure API access vs. locked in a local machine file.*  

---
17.  Explain about data preprocessing tasks

**Data Preprocessing Tasks:** Steps to clean, transform, and prepare raw data for analysis:

1. **Data Cleaning:** Handle missing values, remove duplicates, and correct errors.  
    _Example:_ Filling missing ages or correcting inconsistent entries.
    
2. **Data Integration:** Combine data from multiple sources into a unified dataset.  
    _Example:_ Merging sales data from different regions.
    
3. **Data Transformation:** Convert data into suitable formats or scales.  
    _Example:_ Normalizing numerical features, encoding categorical variables.
    
4. **Data Reduction:** Reduce dataset size without losing essential information.  
    _Example:_ Dimensionality reduction (PCA) or sampling large datasets.
    
5. **Discretization:** Convert continuous data into discrete intervals or bins.  
    _Example:_ Binning ages into ranges (0–18, 19–35, etc.).
    

**Summary:** Preprocessing ensures data is **accurate, consistent, and analysis-ready**, improving model performance and reliability of insights.

---

18.  What are the causes for missing data?

- **Human Error:** Data not entered or recorded incorrectly.  
    _Example:_ Skipped fields in forms.
    
- **System Failures:** Software bugs, crashes, or hardware malfunctions.  
    _Example:_ Sensor or database failures.
    
- **Data Integration Issues:** Mismatched schemas or failed merges from multiple sources.  
    _Example:_ Missing columns when combining datasets.
    
- **Privacy or Non-Response:** Users deliberately omit information.  
    _Example:_ Survey respondents skip sensitive questions.
    
- **Transmission Errors:** Loss during data transfer between systems.  
    _Example:_ Network interruptions causing incomplete logs.

---
19.  How to handle missing data?

Strategies to Handle Missing Data

1. **Ignore Tuples**
    
    - Drop rows with missing fields.
        
    - _Risk_: If many rows are dropped, dataset shrinks and becomes biased.
        
2. **Manual Fill**
    
    - Humans fill in missing values.
        
    - _Risk_: Impractical for large datasets.
        
3. **Global Constant**
    
    - Replace with “unknown” or `0`.
        
    - _Risk_: Distorts analysis (e.g., `0` salary reduces averages).
        
4. **Imputation (Smart Replacement)**
    
    - **Mean/Median/Mode**: Fill missing with overall average or most common value.
        
    - **Class-Based Mean**: Fill using averages within a subgroup (e.g., per `Region`).
        
    - **Most Probable Value**: Predict using models (Bayesian inference, decision trees, regression).
        

---

20.  Explain the scenario for conversion of nominal to numeric?

**Scenario for Converting Nominal to Numeric:**

When a dataset contains **categorical (nominal) variables** that are non-numeric, many machine learning algorithms cannot process them directly. Conversion to numeric format is required for computation.

**Example Scenario:**

- Dataset with a column **“Color”**: `Red, Blue, Green`
    
- **Conversion Techniques:**
    
    1. **One-Hot Encoding:** Create separate binary columns for each category.
        
        `import pandas as pd df = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']}) df_encoded = pd.get_dummies(df, columns=['Color'])`
        
        Result:
			
			  Color_Blue  Color_Green  Color_Red
			0          0            0          1
			1          1            0          0
			2          0            1          0

    1. **Label Encoding:** Assign numeric labels to categories.
        
        `Red → 0, Blue → 1, Green → 2`
        

**Use Case:** Required for algorithms like **linear regression, logistic regression, SVM, and neural networks**, which only process numeric inputs.


---

21.  List the techniques to handle missing data

Strategies to Handle Missing Data

1. **Ignore Tuples**
    
    - Drop rows with missing fields.
        
    - _Risk_: If many rows are dropped, dataset shrinks and becomes biased.
        
2. **Manual Fill**
    
    - Humans fill in missing values.
        
    - _Risk_: Impractical for large datasets.
        
3. **Global Constant**
    
    - Replace with “unknown” or `0`.
        
    - _Risk_: Distorts analysis (e.g., `0` salary reduces averages).
        
4. **Imputation (Smart Replacement)**
    
    - **Mean/Median/Mode**: Fill missing with overall average or most common value.
        
    - **Class-Based Mean**: Fill using averages within a subgroup (e.g., per `Region`).
        
    - **Most Probable Value**: Predict using models (Bayesian inference, decision trees, regression).

---

22.  Explain Binning methods for smoothing data with example?
Types of Binning

| Method                      | How It Works                                                      | Pros                | Cons                                      |
| --------------------------- | ----------------------------------------------------------------- | ------------------- | ----------------------------------------- |
| **Equal-Width**             | Divide range `[A,B]` into N equal intervals, width = `(B-A)/N`    | Simple to implement | Sensitive to outliers and skewed data     |
| **Equal-Depth (Frequency)** | Divide data into N intervals with roughly equal number of samples | Handles skew better | Complex for categorical or small datasets |

---

### b) Example Dataset

## Binning Explained with Formulas

We have sorted values:

`4, 8, 9, 15, 21, 21, 24, 25, 26, 28, 29, 34`

---

### **1. Equal-Width Binning**

**Formula:**

$$\text{Bin Width} = \frac{\text{Max} - \text{Min}}{N} = \frac{B - A}{N}$$
- A = minimum value, B = maximum value, N = number of bins.
    
- Each bin covers an interval: `[A, A+width), [A+width, A+2*width), …`
    

**Step by Step:**

1. Min = 4, Max = 34, N = 3
    

$\text{Width} = \frac{34 - 4}{3} = \frac{30}{3} = 10$

2. Bins:
    

- Bin 1: 4 ≤ x < 14 → 4, 8, 9
    
- Bin 2: 14 ≤ x < 24 → 15, 21, 21
    
- Bin 3: 24 ≤ x ≤ 34 → 24, 25, 26, 28, 29, 34
    

> Notice: Equal-width can lead to uneven counts per bin if data is skewed.

---

### **2. Equal-Depth (Frequency) Binning**

**Formula:**

$\text{Bin Size} = \frac{\text{Total no. of Values}}{\text{Number of bins}} = \frac{n}{N}$

- n = total number of values, N = number of bins.
    
- Each bin has roughly the same number of samples (floor/ceiling used if n not divisible by N).
    

**Step by Step:**

1. n = 12, N = 3 → Bin size = 12/3 = 4 values per bin
    
2. Assign sorted values:
    

- Bin 1 (first 4 values): 4, 8, 9, 15
    
- Bin 2 (next 4 values): 21, 21, 24, 25
    
- Bin 3 (last 4 values): 26, 28, 29, 34
    

> This ensures each bin has equal frequency, even if value ranges differ.

---

### **Observation**

- **Equal-width** bins preserve the numeric range but may have unbalanced counts.
    
- **Equal-depth** bins balance counts but numeric ranges vary.

---

### c) Smoothing Methods Explained

#### 1. **Mean Smoothing**

- Replace **all values in a bin** with the **average (mean)** of that bin.
    
- Reduces variance but moves every value toward the central tendency.
    

Step by step:

**Bin 1: 4, 8, 9, 15**

- Mean = (4 + 8 + 9 + 15)/4 = 9
    
- All values → 9
    

**Bin 2: 21, 21, 24, 25**

- Mean = (21 + 21 + 24 + 25)/4 = 23
    
- All values → 23
    

**Bin 3: 26, 28, 29, 34**

- Mean = (26 + 28 + 29 + 34)/4 = 29
    
- All values → 29
    

|Original Bin|Smoothed (Mean)|
|---|---|
|4, 8, 9, 15|9, 9, 9, 9|
|21, 21, 24, 25|23, 23, 23, 23|
|26, 28, 29, 34|29, 29, 29, 29|

---

#### 2. **Boundary Smoothing**

- Replace **each value** with the **closest bin boundary** (either min or max of the bin).
    
- Preserves extremes, avoids creating new values.
    

Step by step:

**Bin 1: 4, 8, 9, 15** (min=4, max=15)

|Value|Distance to Min|Distance to Max|Smoothed|
|---|---|---|---|
|4|0|11|4|
|8|4|7|4|
|9|5|6|4|
|15|11|0|15|

**Bin 2: 21, 21, 24, 25** (min=21, max=25)

|Value|Distance to Min|Distance to Max|Smoothed|
|---|---|---|---|
|21|0|4|21|
|21|0|4|21|
|24|3|1|25|
|25|4|0|25|

**Bin 3: 26, 28, 29, 34** (min=26, max=34)

|Value|Distance to Min|Distance to Max|Smoothed|
|---|---|---|---|
|26|0|8|26|
|28|2|6|26|
|29|3|5|26|
|34|8|0|34|

|Original Bin|Smoothed (Boundary)|
|---|---|
|4, 8, 9, 15|4, 4, 4, 15|
|21, 21, 24, 25|21, 21, 25, 25|
|26, 28, 29, 34|26, 26, 26, 34|

---

23.  What outlier analysis?

 Outlier Analysis

- Outliers can be detected using **clustering methods**.
    
- Data points that don’t belong to any dense cluster, or lie far from cluster centroids, are considered **outliers**.
    

**Detailed Explanation:**  
Clustering (e.g., **KMeans**) partitions data into $k$ groups. Each cluster has a **centroid** $c = (c_1, c_2, …, c_n)$ representing its “center.”

- For each data point $x = (x_1, x_2, …, x_n)$, the distance to centroid is:
 $d(x, c) = \sqrt{\sum_{i=1}^n (x_i - c_i)^2}$
- If $d(x, c)$ is **much larger** than the typical distances inside the cluster, or if the cluster containing $x$ has **very few members**, then $x$ is flagged as an **outlier**.
    

**Clarified Example:**  
Dataset: [1, 2, 3, 100]

- KMeans with $k=2$ → cluster 1 = {1, 2, 3}, cluster 2 = {100}.
    
- Since {100} forms a **tiny cluster far away**, it is treated as an outlier.
    
```python
from sklearn.cluster import KMeans 
import numpy as np  
data = np.array([[1], [2], [3], [100]]) 
kmeans = KMeans(n_clusters=2, random_state=0).fit(data)  labels = kmeans.labels_ 
centers = kmeans.cluster_centers_  
# find points in smallest cluster unique, 
counts = np.unique(labels, return_counts=True) outlier_cluster = unique[np.argmin(counts)] 
outliers = data[labels == outlier_cluster]  
print("Cluster centers:", centers.ravel())
print("Outliers:", outliers.ravel())
 ```

**Visualization:**

- Dense cluster → bubble of points (1, 2, 3).
    
- Distant single point (100) → lies outside bubble → flagged as **outlier**.

![[Pasted image 20250824203610.png]]

---

24.  What are the causes for occurring inconsistent data with examples?

- **Data Entry Errors:** Different formats or typos entered manually.  
    _Example:_ “NY” vs “New York” for the same state.
    
- **Integration of Multiple Sources:** Conflicting values from merging datasets.  
    _Example:_ Customer birthdate is `01/01/1990` in one table and `02/01/1990` in another.
    
- **Redundant Data:** Duplicate records with differing values.  
    _Example:_ Two records for the same employee with different phone numbers.
    
- **Outdated Information:** Some records not updated after changes.  
    _Example:_ Old address remains while a customer moves.
    
- **Violation of Data Standards or Constraints:** Ignoring rules for valid formats or ranges.  
    _Example:_ Negative values in a field for age or salary.

---
25.  Which of the following is a common method for handling missing data in a dataset?

answer data imputation

A. Data normalization  
B. Data encryption  
C. **Data imputation  
D. Data transformation


---


26.  Which technique is used to detect and correct noisy data?

answer data smoothing 

A. Data visualization  
B. **Smoothing techniques**  
C. Data mining  
D. Hashing

---

27.  Which of the following tools or techniques is typically used to identify duplicate records in a dataset?

answer string matching

A. Data labeling  
B. **String matching or record linkage  
C. Data shuffling  
D. One-hot encoding

---

28.  Which of the following is an example of a data formatting issue that can affect validation

answer inconsistent date formats 

A. Outliers in numerical data  
B. **Inconsistent date formats (e.g., DD/MM/YYYY vs. MM/DD/YYYY)  
C. Null values  
D. Large dataset size

---

29.  Discuss the problems to be considered while data integration

1. **Schema Conflicts:** Different structures, column names, or data types across sources.  
    _Example:_ “DOB” vs “Birth_Date” columns in two datasets.
    
2. **Data Redundancy:** Repeated or overlapping information leading to duplication.  
    _Example:_ Same customer present in multiple systems.
    
3. **Data Inconsistency:** Conflicting values for the same entity.  
    _Example:_ Address differs between sales and support databases.
    
4. **Missing or Incomplete Data:** Some sources lack certain fields.  
    _Example:_ One table has phone numbers, another doesn’t.
    
5. **Heterogeneous Data Sources:** Combining structured, semi-structured, and unstructured data.  
    _Example:_ Merging SQL tables with JSON logs.
    
6. **Data Quality Issues:** Errors, noise, or outdated records in sources affect integration reliability.
    
7. **Scalability & Performance:** Large datasets require efficient merging and conflict resolution strategies.
    

**Summary:** Proper integration requires resolving schema mismatches, removing redundancies, handling missing/inconsistent data, and ensuring quality for analysis.

---

30.  List the techniques to identify redundant data

- **Duplicate Record Detection** – Check for identical rows in a dataset (using `drop_duplicates()` in pandas).
    
- **Correlation Analysis** – Highly correlated attributes (≈1 or -1) may be redundant.
    
- **Feature Importance / Variance Check** – Low variance or constant features add no new info.
    
- **Record Linkage / String Matching** – Detect near-duplicate entries (e.g., "Jon" vs "John").
    
- **Normalization & Standardization** – Helps identify attributes representing the same concept in different units (e.g., height in cm vs meters).
    
- **Schema Matching** – Check overlap when integrating datasets from multiple sources.

---

31.  List the strategies for data transformation

- **Smoothing** – Remove noise using techniques like binning, regression, or clustering.
    
- **Normalization/Scaling** – Transform data into a specific range or scale (e.g., Min-Max, Z-score, Decimal scaling).
    
- **Aggregation** – Summarize by combining values (e.g., average sales per month).
    
- **Generalization** – Replace low-level values with higher-level concepts (e.g., “21 years” → “Young Adult”).
    
- **Attribute Construction (Feature Engineering)** – Create new features from existing ones (e.g., BMI from weight/height).
    
- **Discretization** – Convert continuous data into categorical bins (e.g., marks into grades A/B/C).
    
- **Encoding** – Convert categorical attributes into numeric form (Label encoding, One-hot encoding).
    
- **Logarithmic/Power Transformations** – Reduce skewness and stabilize variance.
    
- **Date/Time Transformation** – Extract useful components (e.g., day, month, weekday).

---


32.  Explain the following with example

a.      min-max normalization

b.     z-score normalization

c.      normalization by decimal scaling

**a. Min-Max Normalization**

- Formula:
    

$v' = \frac{v - \min(A)}{\max(A) - \min(A)} \times (new\_max - new\_min) + new\_min$

- Example: Suppose ages = {10, 20, 30, 40}, we want to scale into [0,1].  
    For v = 30:
    

$v' = \frac{30 - 10}{40 - 10} \times (1 - 0) + 0 = \frac{20}{30} = 0.67$

---

**b. Z-Score Normalization (Standardization)**

- Formula:
    

$v' = \frac{v - \mu}{\sigma}$

where $\mu = mean,$$\sigma = standard deviation.$

- Example: Suppose exam scores = {50, 60, 70}, $\mu = 60,$$\sigma = 10$ 
    For $v = 70$
    

$v' = \frac{70 - 60}{10} = 1$

---

**c. Normalization by Decimal Scaling**

- Formula:
    

$v' = \frac{v}{10^j}$
where j = smallest integer such that max(|v'|) < 1.

- Example: Values = {−500, 200, 900}. Max = 900 → need j=3j=3.  
    For v=200v = 200:
    

$v' = \frac{200}{10^3} = 0.2$

---



33.  explain the concept of data generalization through concept hierarchy?

**Data Generalization through Concept Hierarchy**

- **Concept**: Data generalization is the process of replacing low-level, detailed data with higher-level, more abstract concepts using **concept hierarchies**.
    
- **Why**: It reduces data complexity, highlights meaningful patterns, and prepares data for analysis/mining.
    

### **How it works**

- **Concept Hierarchy**: An ordered set of concepts from specific → generalized.  
    Example:
    
    - **City → State → Country**
        
    - **Day → Month → Year**
        
    - **Marks (0–100) → Grade (A, B, C)**
        

### **Examples**

1. **Location**
    
    - Raw data: `Hyderabad`
        
    - Generalized to: `Telangana` → `India`
        
2. **Time**
    
    - Raw data: `15/08/2025`
        
    - Generalized to: `August 2025` → `2025`
        
3. **Age**
    
    - Raw data: `23, 27, 31`
        
    - Generalized to: `20–30` → `Young Adult`
        

### **Importance**

- Removes unnecessary detail
    
- Provides higher-level insights
    
- Improves efficiency of data mining (patterns emerge clearly)
    

 In short: **Generalization = abstraction via hierarchies → from detailed data to broader, more useful knowledge.**

---

34.  Define data reduction

Data reduction is compressing data into a smaller, more manageable form **without losing significant patterns or meaning**.

35.  List data reduction strategies

- **Dimensionality Reduction** – Reduce number of attributes/features (e.g., PCA, feature selection).
    
- **Numerosity Reduction** – Replace data with smaller representations (e.g., sampling, regression, clustering).
    
- **Data Compression** – Encode data efficiently (lossless or lossy).
    
- **Aggregation** – Summarize data (e.g., daily → monthly totals).
    
- **Discretization & Binning** – Replace continuous values with intervals/bins.

36.  What are the dimensionality reduction techniques

- **Principal Component Analysis (PCA)** – Transforms correlated features into fewer uncorrelated components.
    
- **Linear Discriminant Analysis (LDA)** – Reduces dimensions while preserving class separability.
    
- **Feature Selection** – Keeps only the most relevant attributes (filter, wrapper, embedded methods).
    
- **Autoencoders (Neural Networks)** – Learn compressed latent representations.
    
- **t-SNE / UMAP** – Non-linear techniques for visualization and manifold learning.
    
- **Singular Value Decomposition (SVD)** – Factorizes matrices to lower-rank approximations.

37.  Briefly discuss PCA?
explain with example if needed

**Principal Component Analysis (PCA):**

- **Idea:** Reduce dimensionality by transforming correlated variables into a smaller set of uncorrelated variables called _principal components_.
    
- **How it works:**
    
    1. Standardize data.
        
    2. Compute covariance matrix.
        
    3. Find eigenvalues & eigenvectors.
        
    4. Order eigenvectors by eigenvalues (importance).
        
    5. Project data onto top-k components.
        
- **Result:** Most of the variance (information) is preserved with fewer dimensions.
    
- **Example:** Reducing a dataset with 50 features to 2 principal components for visualization.

38.  What data compression

**Data Compression:** Process of reducing the storage space or transmission cost of data by encoding it more efficiently, while preserving the original information (lossless) or allowing some acceptable loss (lossy).

Example:

- **Lossless:** ZIP, Huffman coding.
    
- **Lossy:** JPEG image compression, MP3 audio.


39.  What are Parametric methods and non parametric methods?

**Parametric Methods**

- Assume data fits a model.
    
- Estimate and store **model parameters**, discard raw data (except outliers).
    
- Example: **Log-linear models** – value at a point in m-D space = product over marginal subspaces.
    

**Non-Parametric Methods**

- No assumptions about data distribution.
    
- Main techniques:
    
    1. **Histograms** – summarize frequency distributions.
        
    2. **Clustering** – group similar data points.
        
    3. **Aggregation** – combine data points into summary statistics.
        
    4. **Sampling** – select representative subset.
        
    5. **Data Cubes** – multidimensional summaries for fast queries.


40.  Define clustering

Grouping a set of data objects into clusters so that objects in the same cluster are more similar to each other than to those in other clusters.  
_Example:_ Grouping customers by purchasing behavior.

41.  What data sampling

## **Definition**

Selecting a smaller, representative subset of the dataset to **reduce storage, computation, and training time** while maintaining essential data characteristics.

---

## **Methods**

### 1. **Random Sampling**

- Pick records **completely at random**.
    
- **Advantage**: Simple, unbiased if dataset is balanced.
    
- **Weakness**: May miss minority/rare classes in skewed data.
    

**Mini Example**  
**Before (Full Data):**

|Feature1|Feature2|Label|
|---|---|---|
|0.5|1.2|0|
|-0.3|0.8|1|
|0.7|-0.4|0|
|-0.1|0.9|1|

**After (Random Sample, 50%)**:

|Feature1|Feature2|Label|
|---|---|---|
|-0.1|0.9|1|
|0.7|-0.4|0|

![[Pasted image 20250824221427.png]]

---

### 2. **Stratified Sampling**

- Data is split into **strata (subgroups)**, and sampling is done **proportionally**.
    
- **Advantage**: Guarantees minority groups are represented.
    
- **Weakness**: Requires knowledge of subgroup labels.
    

**Mini Example**  
Dataset: 2 label=0, 2 label=1 → take 50% → select 1 from each class.

**After (Stratified Sample, 50%)**:

|Feature1|Feature2|Label|
|---|---|---|
|0.5|1.2|0|
|-0.3|0.8|1|

![[Pasted image 20250824221800.png]]

---

### 3. **Systematic Sampling**

- Select **every n-th record** from ordered data.
    
- **Advantage**: Easy to implement, spreads sample evenly.
    
- **Weakness**: Can introduce bias if hidden periodicity exists in data.
    

**Mini Example**  
Step = 2 → pick rows 1, 3, …

**After (Systematic Sample):**

|Feature1|Feature2|Label|
|---|---|---|
|0.5|1.2|0|
|0.7|-0.4|0|
![[Pasted image 20250824221958.png]]

---

## **Comparison Table**

| Method         | How it Works                  | Strength                          | Weakness                 | Best For                              |
| -------------- | ----------------------------- | --------------------------------- | ------------------------ | ------------------------------------- |
| **Random**     | Pick arbitrary rows           | Simple, unbiased in balanced data | Misses rare classes      | Large balanced datasets               |
| **Stratified** | Preserve subgroup proportions | Captures all classes              | Needs class labels       | Classification tasks, imbalanced data |
| **Systematic** | Take every n-th record        | Even spread, simple               | Bias if data is periodic | Time-series, ordered datasets         |



42.  What is data cube aggregation

**Definition**

- A **multidimensional representation** of data used in **OLAP (Online Analytical Processing)**.
    
- Summarizes detailed data across multiple **dimensions** (e.g., _time, region, product_) to reduce complexity and support fast analysis.
    

---

### 2. **Concept**

- **Raw data**: stored at a fine-grained level (e.g., daily sales).
    
- **Aggregation (roll-up)**: combines detailed values into **higher-level summaries** (e.g., yearly sales).
    
- Purpose: Reduce dataset size while keeping **patterns and trends** intact.
    

---

### 3. **How It Works**

- Choose dimensions → group data along those dimensions → apply aggregation (sum, avg, count, etc.).
    
- Example:
    
    - **Dimension**: Time → Levels: Day → Quarter → Year
        
    - **Query**: “What were annual electronics sales by region?”
        
    - Instead of scanning every transaction, OLAP queries the cube’s pre-computed **aggregates**.
        

---

### 4. **Mathematical Representation**

If quarterly sales data is available:

$Annual\_Sales = \sum_{i=1}^{n} Quarterly\_Sales_i$

Where:

- n = number of quarters in a year (usually 4)
    
- $Quarterly\_Sales_i​$ = sales in each quarter
    

---

### 5. **Key Benefit**

- **Efficiency**: Fast analytical queries on large datasets.
    
- **Scalability**: Handles multiple dimensions (time, product, region) without recomputing from raw data each time.

![[Pasted image 20250824222328.png]]

**Example with Separate Tables**:  
Your example: Quarterly electronics sales from 2018–2022 aggregated annually.

**Before Table**:

|Year|Quarter|Sales|
|---|---|---|
|2018|Q1|400000|
|2018|Q2|390000|
|2018|Q3|378000|
|2018|Q4|400000|
|2019|Q1|900000|
|2019|Q2|895000|

**After Table (Annual Aggregation)**:

| Year             | Annual_Sales |
| ---------------- | ------------ |
| 2018             | 1568000      |
| 2019             | 1795000      |

Calculations

- 2018: $400000 + 390000 + 378000 + 400000 = 1568000$
- 2019 (partial): $900000 + 895000 = 1795000$


43.   Why is data integration important in data preprocessing?
ans
C – It helps combine data from multiple sources for unified analysis

A. It deletes unrelated records  
B. It converts data into images  
C. **It helps combine data from multiple sources for unified analysis**  
D. It creates duplicate copies of data

44.  Which of the following is an example of a schema mismatch problem?
ans
B – Different column names for the same attribute in two datasets

A. Missing rows in a dataset  
B. **Different column names for the same attribute in two datasets**  
C. Duplicate records in a single file  
D. Extra white spaces in text fields

45.  What is the purpose of normalization in data transformation?
ans
C – To scale data values into a specific range (e.g., 0 to 1)

A. To reduce data size  
B. To convert data into images  
C. **To scale data values into a specific range (e.g., 0 to 1)**  
D. To create noise in data

46.  . In Pandas, which function is commonly used to merge two datasets based on a common column?
ans
C – `pd.merge()`

A. pd.append()  
B. pd.concat()  
C. pd.merge()  
D. pd.group()

47.  What are the Challenges & Applications of Data Reduction

- **Challenges:** Preserving important information, avoiding loss of patterns, balancing efficiency vs accuracy, handling high-dimensional sparse data.
    
- **Applications:** Faster query processing, efficient storage, visualization of high-dimensional data, improving ML model performance.

48.  Which of the following is a common data reduction technique in data preprocessing?
ans
C – Principal Component Analysis (PCA)

A. Data Augmentation  
B. Data Cleaning  
**C. Principal Component Analysis (PCA)**  
D. Data Shuffling

49.  The main goal of data reduction is to:
ans
C. **Represent the data with less volume but produce similar analytical results**  

A. Remove all noisy data from the dataset  
B. Increase the number of variables  
C. **Represent the data with less volume but produce similar analytical results**  
D. Normalize the data

50.  Explain the concept of dimensionality reduction and how PCA helps in reducing the complexity of large datasets.

**Dimensionality Reduction** → Process of reducing the number of input variables (features) while preserving as much important information as possible. High-dimensional data often causes problems like overfitting, high computation cost, and difficulty in visualization.

**PCA (Principal Component Analysis)** helps by:

- Finding **new axes (principal components)** that capture the maximum variance in data.
    
- These components are **linear combinations** of original features but fewer in number.
    
- By projecting data onto the first few principal components, we keep the **most informative features** and discard the less useful ones.


51.  Discuss the benefits and challenges of using data cube aggregation for summarizing large datasets.

**Data Cube Aggregation** → Pre-computing and storing aggregated data across multiple dimensions (e.g., time, location, product) for fast analysis.

### **Benefits**

- **Faster Querying** → Aggregates (sum, avg, count) are pre-computed → instant retrieval.
    
- **Multi-dimensional Analysis** → Supports OLAP operations (roll-up, drill-down, slice, dice).
    
- **Data Summarization** → Handles massive datasets by storing compact summaries.
    
- **Better Decision-Making** → Enables quick insights at different abstraction levels.
    

### **Challenges**

- **Storage Overhead** → Cube grows exponentially with number of dimensions ("curse of dimensionality").
    
- **Computation Cost** → Building and refreshing the cube is resource-heavy.
    
- **Scalability Issues** → Difficult for real-time updates in very large/streaming datasets.
    
- **Maintenance** → Keeping cubes consistent with raw data requires extra processing.


52.  How does attribute selection contribute to improving the efficiency of machine learning models, and what methods can be used for this process?

**Attribute (Feature) Selection** improves ML efficiency by:

- **Reducing dimensionality** → less data to process, faster training/testing.
    
- **Eliminating noise/irrelevant features** → improves accuracy and generalization.
    
- **Preventing overfitting** → models focus on meaningful patterns only.
    
- **Lowering storage/computation cost** → fewer features = lighter models.
    

### **Methods**

1. **Filter Methods** (independent of ML algorithm)
    
    - Correlation analysis
        
    - Chi-square test
        
    - Information gain / Mutual information
        
2. **Wrapper Methods** (use ML model performance)
    
    - Forward selection
        
    - Backward elimination
        
    - Recursive Feature Elimination (RFE)
        
3. **Embedded Methods** (feature selection built into model)
    
    - LASSO (L1 regularization)
        
    - Decision tree feature importance
        
    - Random forest feature importance


53.  Describe different data sampling techniques and their role in ensuring effective analysis while managing large volumes of data.


**Data Sampling** = selecting a representative subset of data to reduce volume while preserving information for analysis.

### **Techniques**

1. **Random Sampling** – every record has equal chance; unbiased but may miss rare cases.
    
2. **Stratified Sampling** – divide into strata (e.g., gender, region) and sample proportionally; ensures representation of all groups.
    
3. **Systematic Sampling** – pick every _kth_ record; simple, but risk of periodic bias.
    
4. **Cluster Sampling** – divide data into clusters (e.g., cities) and sample whole clusters; efficient for large, distributed datasets.
    
5. **Reservoir Sampling** – for streaming/unknown size datasets; maintains random sample without full storage.
    

### **Role in Analysis**

- Reduces **computational cost**.
    
- Preserves **statistical properties**.
    
- Helps in **scalable model training**.
    
- Enables analysis when full dataset is too large.


54.   What is Data Discretization

**Data Discretization** = process of converting continuous attributes into discrete intervals (bins).

- **Purpose:** simplify data, reduce complexity, improve interpretability, and make algorithms (e.g., decision trees) more efficient.
    
- **Methods:**
    
    1. **Unsupervised:**
        
        - Equal-width binning
            
        - Equal-frequency binning
            
        - Clustering-based (e.g., k-means)
            
    2. **Supervised:**
        
        - Entropy-based discretization
            
        - Chi-square merging
            
- **Benefit:** reduces noise, storage, and enables categorical analysis.


55.   Explain Unsupervised Discretization Methods

## Unsupervised Discretization Methods

### 1. Equal-Width Binning

- **Definition**: Divide the range of values into $k$ intervals of equal width.
- **Formula**: $\text{Width} = \frac{\max(X) - \min(X)}{k}$
- **Steps**:
    1. Determine min and max values.
    2. Calculate bin width.
    3. Assign each value to a bin.

**Detailed Explanation**:  
Equal-width binning splits the data range into $k$ equal-sized intervals. For example, if ages range from 0 to 100 and $k=4$, each bin spans 25 years. Each value is assigned to the bin covering its range. This is simple but may create uneven bin populations for skewed data, potentially leaving some bins empty.

**Example with Separate Tables**:  
Consider ages: [15, 25, 45, 60].

**Before Table**:

|Age|
|---|
|15|
|25|
|45|
|60|

**After Table (Equal-Width, $k=4$)**:

|Age_Bin|
|---|
|[0, 25)|
|[0, 25)|
|[25, 50)|
|[50, 75)|
|**Calculation**:|

- Range: $\max(X) = 60$, $\min(X) = 15$, so $\text{Width} = \frac{60 - 15}{4} = 11.25$.
- Bins: [15, 26.25), [26.25, 37.5), [37.5, 48.75), [48.75, 60].
- Assign: 15 → [15, 26.25), 25 → [15, 26.25), 45 → [37.5, 48.75), 60 → [48.75, 60].

```python 
import pandas as pd

import numpy as np

data = pd.DataFrame({'value': np.random.randint(0, 100, 15)})

data['equal_width_bin'] = pd.cut(data['value'], bins=4)

print(data)
```

![[Pasted image 20250824223736.png]]

### 2. Equal-Frequency Binning (Quantile Binning)

- **Definition**: Each bin contains approximately the same number of data points.
- **Use**: Useful for skewed distributions.

**Detailed Explanation**:  
Equal-frequency binning divides data so each bin has roughly $n/k$ points, where $n$ is the number of data points and $k$ is the number of bins. This balances bin populations, making it robust for skewed data (e.g., income distributions). Boundaries are set at quantiles (e.g., quartiles for $k=4$).

**Example with Separate Tables**:  
Consider ages: [15, 20, 25, 30, 45, 60].

**Before Table**:

|Age|
|---|
|15|
|20|
|25|
|30|
|45|
|60|

**After Table (Equal-Frequency, $k=3$)**:

|Age_Bin|
|---|
|[15, 20]|
|[15, 20]|
|[21, 30]|
|[21, 30]|
|[31, 60]|
|[31, 60]|
|**Calculation**:|

- Sort ages: [15, 20, 25, 30, 45, 60].
- Divide into 3 bins, each with ~2 points: [15, 20], (20, 30], (30, 60].
- Assign: 15, 20 → [15, 20]; 25, 30 → (20, 30]; 45, 60 → (30, 60].

**Comparison Table**:

| Feature          | Equal Width           | Equal Frequency |
| ---------------- | --------------------- | --------------- |
| Bin Size         | Fixed                 | Varies          |
| Distribution     | Can be skewed         | Balanced        |
| Interpretability | Easy                  | Moderate        |
| Outliers         | May create empty bins | More robust     |
 
![[Pasted image 20250824223804.png]]



56.   Explain Clustering-Based Discretization

- **Process:**
    
    1. Apply a clustering algorithm (e.g., k-means, hierarchical).
        
    2. Each cluster groups “similar” values.
        
    3. Replace values in each cluster with its cluster label (discrete bin).
        
- **Example:**  
    Suppose _Age_ = [5, 7, 8, 22, 24, 26, 60, 62].
    
    - K-means (k=3) may form clusters: {5,7,8}, {22,24,26}, {60,62}.
        
    - Discrete bins: Young = 1, Adult = 2, Senior = 3.
        
- **Advantages:** captures natural groupings in data, adaptive.
    
- **Disadvantages:** depends on clustering algorithm choice, may be unstable with noise.



57.   Explain Entropy-Based Discretization


**Entropy-Based Discretization** → uses **class information** to choose split points in continuous attributes by minimizing class impurity (entropy).

- **Process:**
    
    1. Sort continuous values.
        
    2. Consider possible cut points between adjacent values.
        
    3. For each candidate split, compute **information gain** (reduction in entropy after the split).
        
    4. Select the cut that gives **maximum information gain**.
        
    5. Repeat recursively until stopping criteria (e.g., minimal gain threshold or max intervals).
        
- **Example:**  
    Attribute _Age_ with labels {Yes, No}.  
    Possible split: Age ≤ 25 vs. Age > 25.  
    Compute entropy before and after split → if gain is high, discretize there.
    

58.   Advantages & Limitations of Entropy-Based

- **Advantages:**
    
    - Supervised → uses class labels.
        
    - Produces bins that are most useful for prediction.
        
- **Disadvantages:**
    
    - Computationally heavier than unsupervised methods (like equal-width).
        
    - Risk of overfitting if too many bins created.


59.   Which of the following statements best describes equal-frequency binning?
ans 
**c.**      **It ensures each bin has approximately the same number of data points**

a.      It divides the range of values into intervals of equal width.

b.      It assigns each data point to a randomly selected bin.

**c.**      **It ensures each bin has approximately the same number of data points**

d.      It uses class labels to decide where to split the data.

60.   Which method of discretization uses Information Gain to determine the best split point for continuous attributes?
ans
C) **Entropy-based discretization**  

A) Equal-width binning  
B) K-means clustering  
C) **Entropy-based discretization**  
D) Z-score normalization

61.   Explain the difference between supervised and unsupervised discretization methods. Provide one example of each and describe how they work.

- **Supervised Discretization:**
    
    - Uses **class label information** when deciding how to split continuous attributes into intervals.
        
    - Goal: maximize separation between classes.
        
    - Example: **Entropy-based discretization** → Splits attribute values at points that minimize class entropy (maximize information gain). For instance, splitting “age” into {≤30, 31–50, >50} based on how well those intervals separate “disease=yes/no.”
        
- **Unsupervised Discretization:**
    
    - Ignores class labels; partitions are based only on the attribute’s distribution.
        
    - Goal: create intervals that represent the data distribution.
        
    - Example: **Equal-width discretization** → Divides the range of a continuous attribute (e.g., income from 20k–100k) into fixed intervals of equal size (like 20k–40k, 40k–60k, etc.) regardless of labels.

62.   Describe the steps involved in applying clustering-based discretization using the K-Means algorithm. What are the advantages and limitations of this method?

**Supervised vs. Unsupervised Discretization**

- **Supervised Discretization:**
    
    - Uses **class label information** when deciding how to split continuous attributes into intervals.
        
    - Goal: maximize separation between classes.
        
    - Example: **Entropy-based discretization** → Splits attribute values at points that minimize class entropy (maximize information gain). For instance, splitting “age” into {≤30, 31–50, >50} based on how well those intervals separate “disease=yes/no.”
        
- **Unsupervised Discretization:**
    
    - Ignores class labels; partitions are based only on the attribute’s distribution.
        
    - Goal: create intervals that represent the data distribution.
        
    - Example: **Equal-width discretization** → Divides the range of a continuous attribute (e.g., income from 20k–100k) into fixed intervals of equal size (like 20k–40k, 40k–60k, etc.) regardless of labels.
        

**Key Difference:** Supervised focuses on predictive power (uses labels), unsupervised focuses on data representation (ignores labels).

Want me to tabulate this for quick revision?

##### You said:

62.   Describe the steps involved in applying clustering-based discretization using the K-Means algorithm. What are the advantages and limitations of this method?


**Clustering-Based Discretization with K-Means**

**Steps:**

1. **Input data:** Select the continuous attribute(s) to discretize.
    
2. **Apply K-Means:** Run K-Means clustering on the attribute values (1D if single attribute).
    
3. **Form clusters:** Assign each value to its nearest cluster centroid.
    
4. **Convert to intervals:** Sort cluster centroids and replace clusters with intervals (e.g., cluster around 25 → interval [20–30]).
    
5. **Replace values:** Map original values to their corresponding cluster/interval.
    

**Advantages:**

- Captures **natural groupings** in data, not arbitrary splits.
    
- Handles **non-uniform distributions** better than equal-width/height binning.
    
- Can reveal hidden patterns.
    

**Limitations:**

- Requires **K (number of clusters)** to be predefined.
    
- Sensitive to **initial centroids** and outliers.
    
- Computationally heavier than simple discretization methods.
    
- May produce unstable intervals if data distribution changes.

63.   What is entropy-based discretization, and how is Information Gain used to determine split points in continuous data? Provide a brief example to illustrate the process.

**Entropy-Based Discretization**

- A **supervised discretization** method that uses **class labels** to guide how continuous attributes are split.
    
- Goal: find cut points that maximize **class separation**.
    

**How it works (using Information Gain):**

1. Sort values of the continuous attribute.
    
2. Identify potential split points (usually midpoints between adjacent values with different class labels).
    
3. For each candidate split:
    
    - Compute **Entropy(Parent)** = impurity before splitting.
        
    - Compute **Entropy(Left)** and **Entropy(Right)** after the split.
        
    - Calculate **Information Gain = Entropy(Parent) – Weighted Avg(Entropy children)**.
        
4. Choose the split point with the **highest Information Gain**.
    
5. Repeat recursively until stopping criteria (e.g., min gain threshold or max bins).
    

**Example:**  
Attribute = "Age", Class = "Buys Product"

|Age|Buys (Yes/No)|
|---|---|
|22|No|
|25|No|
|28|Yes|
|30|Yes|
|40|Yes|
|42|No|

- Candidate split: between 25 and 28 → Age ≤ 26.5 vs. Age > 26.5.
    
- Compute entropy for parent (mixed Yes/No).
    
- Compute entropy for left group (22,25 → mostly No) and right group (28,30,40,42 → mostly Yes).
    
- If Information Gain is high → keep this split.
    

**Key Point:**  
Entropy-based discretization finds cut points where **class purity improves the most**.


64.   Compare and contrast equal-width and equal-frequency binning in terms of bin distribution, sensitivity to outliers, and interpretability. When would you choose one over the other?


**Equal-Width vs Equal-Frequency Binning**

|Aspect|Equal-Width Binning|Equal-Frequency Binning|
|---|---|---|
|**Bin Distribution**|Divides the data range into intervals of the same size (width).|Ensures each bin has (approximately) the same number of data points.|
|**Sensitivity to Outliers**|Highly sensitive → extreme values stretch the range, causing sparse bins.|Less sensitive → since bins are based on counts, not range.|
|**Interpretability**|Easy to interpret (bins align with numeric ranges).|Harder to interpret (cut points depend on data distribution, not fixed ranges).|
|**Uniformity**|Bins may have very uneven counts if data is skewed.|Bins always balanced in terms of number of records.|

**When to choose:**

- **Equal-Width** → when interpretability matters (e.g., "0–10, 10–20, …"), or data is uniformly distributed.
    
- **Equal-Frequency** → when distribution is skewed and you want balanced representation across bins.

65.   Which method correctly reads data from a CSV file named "sales.csv"?
ans
b) pd.read_csv("sales.csv")

a) pd.open_csv("sales.csv")

b) pd.read_csv("sales.csv")

c) pd.load_csv("sales.csv")

d) pd.csv_reader("sales.csv")

66.   How would you select the "Name" and "Salary" columns from a DataFrame df?
ans
d.      d) df.columns("Name", "Salary")

a.      a) df("Name", "Salary")

b.      b) df[["Name", "Salary"]]

c.      c) df.select("Name", "Salary")

d.      d) df.columns("Name", "Salary")

67.   how missing values are represented

Missing values in pandas are represented as **`NaN` (Not a Number)**.

68.   what is the method for finding missing value?

- To find missing values → **`df.isnull()`** or **`df.isna()`** (boolean mask).
    
- To count missing values → **`df.isnull().sum()`**.

69.   What is the method for removing missing value rows?

To remove rows with missing values → **`df.dropna()`**.

70.   Which of the following best describes the process of extracting useful patterns and knowledge from large datasets?
ans
b.      Data Mining

a.      Data Driven science

b.      Data Mining

c.      Big data

d.      Information retrival

71.   Data preprocessing, cleaning, and feature engineering
ans
a.      **Data Scientist**

a.      **Data Scientist**

b.      b) **Machine Learning Engineer**  
c) **Data Analyst**

c.      d)None

72.   briefly discuss about descriptive statistics

Summarizes and describes data using measures like mean, median, mode (central tendency), variance, standard deviation, range (dispersion), and visual tools (histograms, boxplots). It helps understand the data’s basic structure before deeper analysis.

73.   discuss the importance Exploratory Data Analysis

EDA uncovers patterns, trends, and anomalies, checks data quality, validates assumptions, guides feature selection, and shapes modeling strategies. Without it, analysis risks bias, errors, or missed insights


74.   what are the common EDA techniques

- Summary statistics (mean, median, variance, correlations)
    
- Data visualization (histograms, scatter plots, boxplots, pairplots, heatmaps)
    
- Outlier detection (boxplot, z-score, IQR)
    
- Missing value analysis
    
- Distribution checks (normality, skewness, kurtosis)
    
- Feature relationships (correlation matrices, cross-tabulation).


75.   what is data distribution and explain types of data destributions

**Data Distribution** – Describes how values of a variable are spread. It shows frequency or probability of data points.

- **Normal (Gaussian):** Symmetrical, bell-shaped.
    
- **Uniform:** Equal probability for all values.
    
- **Exponential:** Probability decreases rapidly (e.g., time until failure).
    
- **Binomial:** Outcomes of repeated Bernoulli trials.
    
- **Poisson:** Counts of events in fixed interval.
    
- **Skewed distributions:** Data shifted left/right instead of symmetric.

76.   what are types of anomalies

- **Point anomaly:** Single unusual data point (e.g., extreme outlier).
    
- **Contextual anomaly:** Unusual in specific context (e.g., high temperature in winter).
    
- **Collective anomaly:** Group of related unusual points (e.g., sudden traffic surge).

77.   how to detect outliers

- **Statistical methods:** z-score, modified z-score, IQR rule.
    
- **Visualization:** boxplot, scatter plot, histogram.
    
- **Model-based:** clustering (DBSCAN, k-means), isolation forest, LOF.

78.   explain skewed data distribution


When data is not symmetric around the mean.

- **Right/positive skew:** Long tail on right (income data).
    
- **Left/negative skew:** Long tail on left (age at retirement).
    
- Skewness impacts mean/median comparison and modeling assumptions.

79.   **Right-skewed distribution**

- Tail extends to the right; most values concentrated on the left.
    
- _Example:_ Income, house prices.


80.   **Left-skewed distribution**

- Tail extends to the left; most values concentrated on the right.
    
- _Example:_ Exam scores where most students score high.

81.   **Symmetrical distribution**

- Both sides of the distribution mirror each other; mean ≈ median ≈ mode.
    
- _Example:_ Normal distribution of heights

82.   **Write python code to Plot Histogram – Distribution of Marks**

```python
import pandas as pd
import matplotlib.pyplot as plt

marks = [55, 67, 78, 45, 90, 88, 76, 65, 80, 70]
plt.hist(marks, bins=5, color='skyblue', edgecolor='black')
plt.title("Histogram of Marks")
plt.xlabel("Marks")
plt.ylabel("Frequency")
plt.show()

```

83.   **Write python code to Plot Boxplot – Check for Outliers**

```python
plt.boxplot(marks)
plt.title("Boxplot of Marks")
plt.ylabel("Marks")
plt.show()

```


84.   **Write python code to Scatter Plot – Relationship between Marks and Attendance**

```python 
attendance = [80, 90, 85, 70, 95, 92, 88, 75, 90, 85]
plt.scatter(attendance, marks, color='green')
plt.title("Marks vs Attendance")
plt.xlabel("Attendance")
plt.ylabel("Marks")
plt.show()

```

85.   **What are the data types in pandas?**

- **Numeric:** `int64`, `float64`
    
- **Categorical / Object:** `object` (strings), `category`
    
- **Boolean:** `bool`
    
- **Datetime:** `datetime64[ns]`
    
- **Timedelta:** `timedelta[ns]`

86.   **What is a series and dataframe in pandas?**

- **Series:** 1D labeled array, single column.
    
    ```python
    s = pd.Series([10, 20, 30])
    ```
    
- **DataFrame:** 2D labeled data structure (rows × columns).
    
    ```python
    df = pd.DataFrame({'A':[1,2],'B':[3,4]})
    ```


87.   **How to read the data from online**

```python
url = "https://example.com/data.csv"
df = pd.read_csv(url)

```

88.   Explain the following attributes and it’s imporatance in pandas

a.      Dtypes

b.      Columns

c.      Index


- **dtypes:** Shows data type of each column; helps in type-specific operations.
    
- **columns:** Lists column names; useful for selecting, renaming, or dropping columns.
    
- **index:** Row labels; used for selection, alignment, and slicing.


89.   Differentiate describe() and info() in pandas

|Function|Purpose|Output|
|---|---|---|
|`describe()`|Statistical summary of numeric columns|Count, mean, std, min, 25%, 50%, 75%, max|
|`info()`|General overview of DataFrame|Index dtype, column names, non-null counts, memory usage|

|       | **Make** | **Colour** | **Odometer (KM)** | **Doors** | **Price**  |
| ----- | -------- | ---------- | ----------------- | --------- | ---------- |
| **0** | Toyota   | White      | 150043            | 4         | $4,000.00  |
| **1** | Honda    | Red        | 87899             | 4         | $5,000.00  |
| **2** | Toyota   | Blue       | 32549             | 3         | $7,000.00  |
| **3** | BMW      | Black      | 11179             | 5         | $22,000.00 |
| **4** | Nissan   | White      | 213095            | 4         | $3,500.00  |
| **5** | Toyota   | Green      | 99213             | 4         | $4,500.00  |
| **6** | Honda    | Blue       | 45698             | 4         | $7,500.00  |
| **7** | Honda    | Blue       | 54738             | 4         | $7,000.00  |
| **8** | Toyota   | White      | 60000             | 4         | $6,250.00  |
| **9** | Nissan   | White      | 31600             | 4         | $9,700.00  |


90.   Write python code Load above dataset into dataframe, calculate mean of “Odometer (KM)”  and sum of “Doors”
```python
data = {
    "Make": ["Toyota","Honda","Toyota","BMW","Nissan","Toyota","Honda","Honda","Toyota","Nissan"],
    "Colour": ["White","Red","Blue","Black","White","Green","Blue","Blue","White","White"],
    "Odometer (KM)": [150043,87899,32549,11179,213095,99213,45698,54738,60000,31600],
    "Doors": [4,4,3,5,4,4,4,4,4,4],
    "Price": [4000,5000,7000,22000,3500,4500,7500,7000,6250,9700]
}

df = pd.DataFrame(data)

# Mean of Odometer and sum of Doors
print("Mean Odometer:", df["Odometer (KM)"].mean())
print("Sum of Doors:", df["Doors"].sum())
```

91.   Explain the difference between .loc & .iloc with example

```python
# .loc -> label-based indexing (row/column names)
print(df.loc[0, "Make"])  # Toyota
# .iloc -> integer position-based indexing
print(df.iloc[0, 0])      # Toyota
```

|       | **Make** | **Colour** | **Odometer** | **Doors** | **Price** |
| ----- | -------- | ---------- | ------------ | --------- | --------- |
| **0** | Toyota   | White      | 150043.0     | 4.0       | $4,000    |
| **1** | Honda    | Red        | 87899.0      | 4.0       | $5,000    |
| **2** | Toyota   | Blue       | NaN          | 3.0       | $7,000    |
| **3** | BMW      | Black      | 11179.0      | 5.0       | $22,000   |
| **4** | Nissan   | White      | 213095.0     | 4.0       | $3,500    |
| **5** | Toyota   | Green      | NaN          | 4.0       | $4,500    |
| **6** | Honda    | NaN        | NaN          | 4.0       | $7,500    |
| **7** | Honda    | Blue       | NaN          | 4.0       | NaN       |
| **8** | Toyota   | White      | 60000.0      | NaN       | NaN       |
| **9** | NaN      | White      | 31600.0      | 4.0       | $9,700    |

92.   Find the missing values of each column take appropriate action includes fill with mean or mode or median or constant or drop the rows analyse the scenario

```python

data_missing = {
    "Make": ["Toyota","Honda","Toyota","BMW","Nissan","Toyota","Honda","Honda","Toyota",np.nan],
    "Colour": ["White","Red","Blue","Black","White","Green",np.nan,"Blue","White","White"],
    "Odometer": [150043.0,87899.0,np.nan,11179.0,213095.0,np.nan,np.nan,np.nan,60000.0,31600.0],
    "Doors": [4.0,4.0,3.0,5.0,4.0,4.0,4.0,4.0,np.nan,4.0],
    "Price": ["$4,000","$5,000","$7,000","$22,000","$3,500","$4,500","$7,500",np.nan,np.nan,"$9,700"]
}
df_miss = pd.DataFrame(data_missing)

# Fill numeric columns
df_miss["Odometer"].fillna(df_miss["Odometer"].mean(), inplace=True)
df_miss["Doors"].fillna(df_miss["Doors"].median(), inplace=True)

# Fill categorical columns
df_miss["Make"].fillna(df_miss["Make"].mode()[0], inplace=True)
df_miss["Colour"].fillna("Unknown", inplace=True)
df_miss["Price"].fillna("$0", inplace=True)

```
93.   Add a new column to the above  dataset with constant value 10
```python
df_miss["New_Column"] = 10
```

94.    Group the data based on “make” and perform sum on Doors

```python
doors_sum = df_miss.groupby("Make")["Doors"].sum()
print("\nSum of Doors by Make:\n", doors_sum)
```

95.   Filter the rows where “make == Honda” and display

```python
honda_rows = df_miss[df_miss["Make"] == "Honda"]
print("\nHonda Rows:\n", honda_rows)
```