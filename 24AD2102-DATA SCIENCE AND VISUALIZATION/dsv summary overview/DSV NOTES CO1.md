Q what is data science ?  

- An interdisciplinary field combining statistics, computer science, and domain knowledge.

- Involves extracting insights and knowledge from structured and unstructured data.

- Core techniques: machine learning, data mining, predictive analytics

<p align="center" style="font-size:18px">(or)</p>

- Data science is the study of data that helps us derive useful insight for business decision making.

- Data Science is all about using tools, techniques, and creativity to uncover insights hidden within data.

- It combines math, computer science, and domain expertise to tackle real-world challenges in a variety of fields.
---

Why is Data Science Important?

- Helps make data-driven decisions

- Drives innovation across industries

- Powers technologies like AI, recommendation systems, fraud detection, etc.

- Generates value from vast amounts of data

**Steps to Data science**

```mermaid
flowchart TD
    A[Data Collection] --> B[Data Cleaning]
    B --> C[Data Analysis]
    C --> D[Data Visualization]
    D --> E[Decision-Making]

```


**Data Collection:** Gathering raw data from various sources, such as databases, sensors, or user interactions.

**Data Cleaning:** Ensuring the data is accurate, complete, and ready for analysis.

**Data Analysis:** Applying statistical and computational methods to identify patterns, trends, or relationships.

**Data Visualization:** Creating charts, graphs, and dashboards to present findings clearly.

**Decision-Making:** Using insights to inform strategies, create solutions, or predict outcomes.

----

Q Difference Between Data Science and Data Visualization?

**Data Science:** Full pipeline—collecting, cleaning, analyzing, modeling data, extracting insights, and making predictions or decisions. Encompasses stats, ML, programming, and domain knowledge.

**Data Visualization:** Subset/tool of data science—focuses on representing data visually (charts, graphs, dashboards) to communicate insights effectively. Doesn’t inherently analyze or predict.



<p align="center" style="font-size:18px">(or)</p>

**Data Science:** Data science is study of data. It involves developing methods of recording, storing, and analyzing data to extract useful information

**Data Visualization:** Data visualization is the graphical representation of information and data in a pictorial or graphical format(Example: charts, graphs, and maps).

---

<p align="center" style="font-size:24px"><b>IMPORTANCE OF DATA SCIENCE</b></p>




<p align="center" style="font-size:24px"><b>ROLES IN DATA SCIENCE<b></p>

### 1. Data Scientist

**Responsibilities:**

- Analyze and interpret complex data sets to extract insights
    
- Build predictive models and machine learning algorithms
    
- Preprocess, clean, and engineer features
    
- Communicate findings to non-technical stakeholders
    

### 2. Machine Learning Engineer

**Responsibilities:**

- Develop and deploy machine learning models
    
- Optimize models for performance and scalability
    
- Collaborate with data scientists to operationalize work
    
- Manage and maintain model infrastructure
    

### 3. Data Analyst

**Responsibilities:**

- Analyze data to provide actionable insights
    
- Create visualizations and reports for decision-making
    
- Identify trends and patterns in data
    
- Collaborate with business stakeholders to understand data needs
    

### 4. Data Engineer

**Responsibilities:**

- Build and maintain data pipelines and ETL processes
    
- Manage data infrastructure and databases
    
- Ensure data quality, reliability, and availability
    
- Support data scientists and analysts with clean, accessible data
    

### 5. Business Intelligence (BI) Analyst

**Responsibilities:**

- Create dashboards and reports for business performance tracking
    
- Design data visualization tools for end-users
    
- Identify key performance indicators (KPIs) and metrics
    
- Collaborate with business teams to support decision-making
    

### 6. Data Architect

**Responsibilities:**

- Design and maintain data architectures
    
- Define data storage, integration, and processing strategies
    
- Ensure data security and compliance
    
- Collaborate with data engineers to implement data solutions
    

### 7. Statistician

**Responsibilities:**

- Apply statistical techniques to analyze data
    
- Conduct hypothesis testing and experiments
    
- Design surveys and experiments to gather data
    
- Provide statistical insights to support decision-making
    

### 8. AI/ML Researcher

**Responsibilities:**

- Conduct research to advance machine learning and AI
    
- Develop novel algorithms and models
    
- Publish research papers and contribute to conferences
    
- Collaborate with data scientists and engineers on cutting-edge solutions
    

### 9. Quantitative Analyst (Quant)

**Responsibilities:**

- Apply quantitative and mathematical methods to financial data
    
- Develop trading strategies and risk models
    
- Analyze market data to inform investment decisions
    
- Implement algorithms for trading and risk management
    

### 10. Chief Data Officer (CDO)

**Responsibilities:**

- Set organizational data strategy and governance
    
- Oversee data management, privacy, and compliance
    
- Align data initiatives with business goals
    
- Manage data-related teams and resources

---

<p align="center" style="font-size:24px"><b>DATA SCIENCE WORK FLOW AND LIFE CYCLE</b></p>

The Data Science lifecycle is designed for Big Data problems and data science projects. The cycle is iterative to represent real project. To address the distinct requirements for performing analysis on Big Data, step – by – step methodology is needed to organize the activities and tasks involved with acquiring, processing, analyzing, and repurposing data


```mermaid
flowchart TD
    A[Problem Definition] --> B[Data Collection]
    B --> C[Data Cleaning and Preprocessing]
    C --> D[Exploratory Data Analysis]
    D --> E[Feature Engineering]
    E --> F[Modeling]
    F --> G[Evaluation]
    G --> H[Deployment]
    H --> I[Monitoring and Maintenance]
```


- **Problem Definition:** Clarify business/analytical objective.
    
- **Data Collection:** Gather data from internal and external sources.
    
- **Data Cleaning & Preprocessing:** Handle missing values, outliers, formatting, and normalization.
    
- **Exploratory Data Analysis (EDA):** Understand distributions, correlations, patterns, and anomalies.
    
- **Feature Engineering:** Create, select, or transform features to improve model performance.
    
- **Modeling:** Apply statistical or ML algorithms to build predictive or descriptive models.
    
- **Evaluation:** Assess model performance using metrics, cross-validation, and testing.
    
- **Deployment:** Integrate model into production or decision-making systems.
    
- **Monitoring & Maintenance:** Track performance, update models with new data, ensure reliability.

<p align="center" style="font-size:18px">(or)</p>

![[Pasted image 20250815131505.png]]

# Data Science Project Workflow

## 1. Understanding the Problem
- The data science team learns and investigates the problem.
- Develops context and understanding.
- Identifies data sources needed and available for the project.
- Formulates initial hypotheses that can be later tested with data.

## 2. Gathering Relevant Data
- Steps to explore, preprocess, and condition data prior to modeling and analysis.
- Requires an analytic sandbox where the team executes, loads, and transforms data.
- Data preparation tasks are iterative and not necessarily in a fixed order.
- **Common tools:** Hadoop, Alpine Miner, OpenRefine, etc.

## 3. Data Preparation
- Team explores data to learn relationships between variables.
- Selects key variables and the most suitable models.
- Develops datasets for training, testing, and production purposes.
- Builds and executes models based on planning.
- **Common tools:** MATLAB, STATISTICA.

## 4. Feature Engineering and Feature Extraction
- Develops datasets for testing, training, and production.
- Assesses whether existing tools suffice or if a more robust environment is needed.
- **Free/Open-source tools:** R, PL/R, Octave, WEKA.
- **Commercial tools:** MATLAB, STATISTICA.

## 5. Model Building and Deployment
- Executes models and compares outcomes against success/failure criteria.
- Articulates findings and outcomes to team members and stakeholders, considering warnings and assumptions.
- Identifies key findings, quantifies business value, and develops a narrative to summarize and convey results.

---

<p align="center" style="font-size:24px"><b>DIFFERENCE BETWEEN DATA SCIENCE AND BIG DATA</b></p>

- **Data Science:** Focused on **analyzing data** to extract insights, build models, and make decisions. Uses statistics, ML, and visualization.
    
- **Big Data:** Refers to **massive, complex datasets** (volume, velocity, variety, veracity) that traditional tools can’t handle efficiently. Focus is on **storage, processing, and management**.

<p align="center" style="font-size:18px">(or)</p>

**Data Science:**

1. Extracts insights from structured and unstructured data.
    
2. Uses statistics, machine learning, and visualization.
    
3. Focuses on solving problems and making predictions.
    
4. Works with datasets of any size, not necessarily huge.
    

**Big Data:**

1. Deals with massive, high-velocity, and diverse datasets.
    
2. Focuses on storage, processing, and management.
    
3. Enables scalable analysis, often using distributed systems.
    
4. Infrastructure-centric; insights require additional tools like data science.

---

<p align="center" style="font-size:24px"><b>DATA SCIENCE AND ARTIFICIAL INTELLIGENCE</b></p>

![[Pasted image 20250815123836.png]]

---

<p align="center" style="font-size:24px"><b>DATA SCIENCE TOOLS AND TECHNOLOGIES</b></p>

- **Programming Languages:** Python, R, SQL, Julia.
    
- **Data Handling & Analysis:** Pandas, NumPy, Spark, Hadoop.
    
- **Visualization:** Matplotlib, Seaborn, Tableau, Power BI, Plotly.
    
- **Machine Learning & AI:** Scikit-learn, TensorFlow, PyTorch, XGBoost, Keras.
    
- **Big Data & Cloud:** Hadoop, Spark, AWS, Azure, Google Cloud.
    
- **Databases:** MySQL, PostgreSQL, MongoDB, Cassandra.
    
- **Collaboration & Versioning:** Git, Jupyter Notebooks, VS Code.
    
- **ETL & Data Pipelines:** Apache Airflow, Talend, Luigi.

| Toolbox        | Purpose                          | Example Use Case                           |
|----------------|----------------------------------|-------------------------------------------|
| NumPy          | Numerical computing              | Fast array operations, linear algebra     |
| Pandas         | Data manipulation and analysis   | DataFrames for tabular data               |
| Matplotlib     | Data visualization (2D plots)    | Line graphs, bar charts, scatter plots   |
| Seaborn        | Statistical data visualization   | Boxplots, correlation graphs              |
| Scikit-learn   | Machine learning                 | Classification, regression, clustering   |
| TensorFlow     | Deep learning                     | Neural networks and deep learning models |
| Statsmodels    | Statistical modeling             | Regression, hypothesis testing           |
| Plotly         | Interactive visualization        | Dashboards, real-time plots               |


---

<p align="center" style="font-size:24px"><b>DATA SCIENCE APPLICATIONS</b></p>

**Healthcare:** Data science improves patient outcomes by using predictive analytics to detect diseases early, creating personalized treatment plans and optimizing hospital operations for efficiency.

**Finance:** Data science helps detect fraudulent activities, assess and manage financial risks, and provide tailored financial solutions to customers.

**Retail:** Data science enhances customer experiences by delivering targeted marketing campaigns, optimizing inventory management, and forecasting sales trends accurately.

**Technology:** Data science powers cutting-edge AI applications such as voice assistants, intelligent search engines, and smart home devices.

**Transportation:** Data science optimizes travel routes, manages vehicle fleets effectively, and enhances traffic management systems for smoother journeys.

**Manufacturing:** Data science predicts potential equipment failures, streamlines supply chain processes, and improves production efficiency through data-driven decisions.

**Energy:** Data science forecasts energy demand, optimizes energy consumption, and facilitates the integration of renewable energy resources.

**Agriculture:** Data science drives precision farming practices by monitoring crop health, managing resources efficiently, and boosting agricultural yields.

<p align="left" style="font-size:16px">examples of applications</p>

--- 

**Fraud Detection **

- **Goal:** Identify fraudulent activity **before it causes major damage**.
    
- **Approach:** Analyze historical data to detect patterns and anomalies.
    
- **Challenges:** Real-time detection is **harder than post-fact**, requires **high precision**.
    
- **Trade-offs:** Both false positives (innocent flagged) and false negatives (fraud missed) are costly.
    
- **Tech:** Stream processing, anomaly detection models, real-time dashboards, ML algorithms.

---

**Recommender Systems:**

- **Purpose:** Deliver **personalized suggestions** to users.
    
- **Impact:** Boosts sales, click-throughs, conversions.
    
- **Examples:**
    
    - **Netflix:** ~$1B/year value from recommendations.
        
    - **Amazon:** 20–35% annual sales lift.
        
- **Technique:** Collaborative filtering at scale, content-based methods, hybrid approaches.

---

**Patient Readmission Prediction:**

- **Goal:** Identify why patients return to the hospital.
    
- **Benefits:** Cut costs, improve population health.
    
- **Focus:** Understand **underlying causes** for specific populations.
    
- **Data:** Integrate multiple sources—EHRs, socioeconomic info, genetics, patient history.
    
- **Approach:** Analyze correlations between readmissions and health/social factors to enable targeted interventions.

---

**Smart Cities:**

- **Definition:** Use data + ICT to optimize urban living.
    
- **Goals:**
    
    - Plan communities efficiently
        
    - Manage infrastructure/assets effectively
        
    - Reduce operational costs
        
    - Use open data to engage citizens

---

<p align="center" style="font-size:24px"><b>HOLISTIC APPROACH TO DATA SCIENCE</b></p>

![[Pasted image 20250815123547.png]]



---

### import
In Python, the `import` statement brings external modules or libraries into your code, giving access to their functions, classes, and variables so you don’t have to rewrite common functionality.

Examples:

```python
import math         # full module 
from math import pi # specific item 
import numpy as np   # alias for convenience
```

It’s essential for code reuse, modularity, and leveraging Python’s ecosystem.

---

### dir Function in Python

The `dir()` function is used to list all the attributes (variables, functions, classes, etc.) of a module or object. It helps you explore what a module contains.

Example with the `math` module

```python
import math
print(dir(math))
```

---

<p align="center" style="font-size:24px"><b>NUMPY</b></p>

What is NumPy?

NumPy (short for Numerical Python ) is one of the most fundamental libraries in Python for scientific computing. It provides support for large, multi-dimensional arrays and matrices along with a collection of mathematical functions to operate on arrays.

<p align="center" style="font-size:16px">(or)</p>

NumPy provides high-performance tools for working with arrays and matrices. Unlike standard Python lists, it supports multi-dimensional data and a rich collection of mathematical operations, making it a foundational library in the data science ecosystem.

It provides support for:

- Multi-dimensional arrays (called ndarray)

- Mathematical operations (linear algebra, statistics, etc.)

- Vectorization for fast computation (avoids Python loops)


#### CREATING AN ARRAY 

 NumPy `array()` Function

The `numpy.array()` function is used to create a NumPy array from a Python list, tuple, or other sequence-like objects. It converts data into an `ndarray` for efficient numerical operations.


Syntax 


```python

import numpy as np                                       
arr = np.array ( [1,2,3,4,5])  
# Displaying ID array
print (arr)
```
**output:**
```
[1 2 3 4 5]
```

```python
import numpy as np
arr = np.array ( [1,2,3,4,5])
# Displaying ID array
for item in arr:
    print (item)
```

#### Understanding NumPy Array Indexing

In NumPy, you can access array elements using **positive indices** (from start) or **negative indices** (from end).

 Example

```python
import numpy as np

arr = np.array([10, 25, 13, 64, 57])

# Positive indexing
print(arr[0])  # 10
print(arr[1])  # 25
print(arr[2])  # 13
print(arr[3])  # 64
print(arr[4])  # 57

# Negative indexing
print(arr[-1])  # 57
print(arr[-2])  # 64
print(arr[-3])  # 13
print(arr[-4])  # 25
print(arr[-5])  # 10


# Access all elements using a loop
for index in range(len(arr)):
    print(arr[index])

```
**output:**

```
10
25
13
64
57

57
64
13
25
10

10
25
13
64
57

```
#### Creating Arrays of Zeros and Ones in NumPy

- `np.zeros(n)` → Creates a NumPy array of size `n` filled with zeros (`0.0`).
    
- `np.ones(n)` → Creates a NumPy array of size `n` filled with ones (`1.0`).
    

> Both are basic array-creation functions used for initializing arrays quickly, often for placeholders or computations.

```python
import numpy as np

# Create an array with five zeros
zeros_array = np.zeros(5)

# Create an array with five ones
ones_array = np.ones(5)

print(zeros_array)  # [0. 0. 0. 0. 0.]
print(ones_array)   # [1. 1. 1. 1. 1.]

```

**Key Point:**
    
- Both return arrays of type `float` by default.
    
You can also specify a different `dtype` if needed, e.g., `np.zeros(5, dtype=int)`.


#### Slicing of an Array in NumPy

Slicing allows you to **access a subset of elements** from a NumPy array.

 Syntax
 
```python

array[start:stop:step]

```

start: Index to begin the slice (inclusive, default 0)

stop: Index to end the slice (exclusive)

step: Step size between elements (default 1)

Examples
```python
import numpy as np

arr = np.array([10, 20, 30, 40, 50, 60])

# Slice from index 1 to 4 (excluding 4)
print(arr[1:4])  # [20 30 40]

# Slice from start to index 3
print(arr[:3])   # [10 20 30]

# Slice from index 2 to end
print(arr[2:])   # [30 40 50 60]

# Slice every 2nd element
print(arr[::2])  # [10 30 50]

# Reverse array
print(arr[::-1]) # [60 50 40 30 20 10]

```

Key Points:

- Omitting start assumes 0.

- Omitting stop goes to the end.

- Negative step reverses the order.


#### Negative Indexing to Access Elements in a 2D Array

Negative indices count from the end (`-1` = last element, `-2` = second last, etc.).

 Example:
```python
import numpy as np

a = np.array([[10, 20, 30],
              [40, 50, 60],
              [70, 80, 90]])

print("Matrix a:\n", a)

# Last row
print("Last row:", a[-1])

# Last element of last row
print("Last element of last row:", a[-1, -1])

# Second last row
print("Second last row:", a[-2])

# Second last element of first row
print("Second last element of first row:", a[0, -2])
````

 Output:

```
Matrix a:
[[10 20 30]
 [40 50 60]
 [70 80 90]]

Last row: [70 80 90]
Last element of last row: 90
Second last row: [40 50 60]
Second last element of first row: 20
```

 Dummy Version:

Think of **`-1` as pointing from the back**.  
So `a[-1]` = last row, `a[-1, -1]` = bottom-right corner.




#### Mathematical Operations on NumPy Arrays

NumPy allows element-wise mathematical operations directly on arrays.

 Example
```python
import numpy as np

k = np.array([1, 2, 3, 4, 5])
print("Original array:", k)

# Addition
k = k + 5
print("After addition operation:", k)

# Subtraction
k = k - 5
print("After subtraction operation:", k)

# Multiplication
k = k * 5
print("After multiplication:", k)

# Division
k = k / 5
print("After division:", k)
```

output:
```
Original array: [1 2 3 4 5]
After addition operation: [ 6  7  8  9 10]
After subtraction operation: [1 2 3 4 5]
After multiplication: [ 5 10 15 20 25]
After division: [1. 2. 3. 4. 5.]


```

#### Searching Elements in a NumPy Array

You can use `np.where()` to find the indices of elements that meet a condition.

Example
```python
import numpy as np

arr = np.array([10, 20, 30, 40, 50, 60])

# Find indices where element is greater than 30
indices = np.where(arr > 30)
print(indices)          # (array([3, 4, 5]),)

# Get the matching elements
print(arr[indices])     # [40 50 60]

```

**Key Points:**

- `np.where(condition)` returns a tuple of index arrays.
    
- You can use these indices to directly extract matching elements.
    
- Useful for filtering arrays without loops.

#### Filtering a NumPy Array

Filtering an array means selecting elements that meet a certain condition using **boolean indexing**.

Example

```python
import numpy as np

arr = np.array([10, 25, 30, 45, 60])

# Create a boolean mask for elements greater than 30
mask = arr > 30
print(mask)        # [False False False  True  True]

# Use the mask to filter elements
filtered = arr[mask]
print(filtered)    # [45 60]
```
#### Checking Whether a NumPy Array is Empty

You can check if an array is empty by using the `.size` attribute, which returns the total number of elements.

Example 1 – Non-empty array
```python
import numpy as np

arr = np.array([1, 2, 3, 4, 5])

if arr.size == 0:
    print("Array is empty")
else:
    print("Array is not empty")

print(arr)
```
Output:
```
Array is not empty
[1 2 3 4 5]
```
Example 2 – Empty array
```python
import numpy as np
arr = np.array([])
if arr.size == 0:
    print("Array is empty")
else:
    print("Array is not empty")
print(arr)
```
Output:

```
Array is empty
[]
```
Key Point:

- .size returns the number of elements in the array; 0 means the array is empty.

#### Creating and Displaying a 2D NumPy Array

 Example 1 – Direct print
```python
import numpy as np

# Create a 2D array
arr = np.array([[1, 2, 3], [4, 5, 6]])

# Display entire 2D array
print(arr)
```

output;
```
[[1 2 3]
 [4 5 6]]
```

Example 2 – Row by row

``` python
import numpy as np

arr = np.array([[1, 2, 3], [4, 5, 6]])

# Display each row
for row in arr:
    print(row)

```

output:
```
[1 2 3]
[4 5 6]

```

**Key Point:**

- Printing the array directly shows it in matrix form.
    
- Iterating with a loop prints each row separately.

#### NumPy `ndim` Attribute

The `.ndim` attribute returns the **number of dimensions** (axes) of a NumPy array.

 Example
```python
import numpy as np

# 1D array
A = np.array([1, 2, 3, 4])
print("Dimension of A is:", A.ndim)

# 2D array
B = np.array([[1, 2, 3], [4, 5, 6]])
print("Dimension of B is:", B.ndim)

```

output:
```
Dimension of A is: 1
Dimension of B is: 2

```

**Key Point:**

- `.ndim` gives how many dimensions an array has.
    
- Example: 1D (vector), 2D (matrix), 3D (tensor).


#### NumPy `size` Attribute

The `.size` attribute returns the **total number of elements** in a NumPy array.

Example 1 – 1D array
```python
import numpy as np

Arr = np.array([1, 2, 3, 4])
print(Arr.size)

```

output:
```
4
```

 Example 2 – 2D array

```python
import numpy as np 

Arr = np.array([[1, 2, 3, 4], [5, 6, 7, 8]]) 
print("Number of elements are:", Arr.size)
```

output:
```
8
```

#### Shape and Reshape in NumPy

`.shape` Attribute
- Returns the **dimensions (rows, columns, …)** of an array as a tuple.

`.reshape()` Method
- Changes the shape of an array **without changing its data**.



Example
```python
import numpy as np

# Create a 2D array
Arr = np.array([[1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12]])

print("Original shape:", Arr.shape)   # (3, 4)

# Reshape to 2 rows × 6 columns
Arr1 = Arr.reshape(2, 6)
print("Reshaped to (2,6):")
print(Arr1)

# Reshape to 1 row × 12 columns
Arr2 = Arr.reshape(1, 12)
print("Reshaped to (1,12):")
print(Arr2)

# Reshape to 12 rows × 1 column
Arr3 = Arr.reshape(12, 1)
print("Reshaped to (12,1):")
print(Arr3)

```
output:

```
Original shape: (3, 4)

Reshaped to (2,6):
[[ 1  2  3  4  5  6]
 [ 7  8  9 10 11 12]]

Reshaped to (1,12):
[[ 1  2  3  4  5  6  7  8  9 10 11 12]]

Reshaped to (12,1):
[[ 1]
 [ 2]
 [ 3]
 [ 4]
 [ 5]
 [ 6]
 [ 7]
 [ 8]
 [ 9]
 [10]
 [11]
 [12]]

```

**Key Points:**

- `.shape` shows the current dimensions.
    
- `.reshape(a, b)` rearranges elements into the given dimensions.
    
- The total number of elements must remain the same (`rows × cols = total elements`).


#### Creating an Identity Matrix

```python
import numpy as np

# Creates an identity matrix (must be square)
x = np.eye(5)
print(x)
```

**Output:**

```
[[1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1.]]
```


#### Using `np.arange()`

`np.arange()` works like Python’s built-in `range()`, but returns a NumPy array.

**Syntax:**

```python
np.arange([start,] stop[, step], dtype=None)
```



 Example 1 – Default step (1)

```python
import numpy as np

x = np.arange(1, 10)   # from 1 to 9
print(x)
```

**Output:**

```
[1 2 3 4 5 6 7 8 9]
```



 Example 2 – Custom step (2)

```python
import numpy as np

x = np.arange(1, 10, 2)   # from 1 to 9 with step 2
print(x)
```

**Output:**

```
[1 3 5 7 9]
```


#### linspace()

- Returns evenly spaced numbers over a specified interval.  
- Unlike `arange()`, you **specify the number of points**, not the step.

 Syntax
```python
numpy.linspace(start, stop, num=50, endpoint=True, dtype=None)
````

- `start` → starting value
    
- `stop` → ending value
    
- `num` → number of evenly spaced samples (default = 50)
    
- `endpoint` → if True (default), includes `stop`; if False, excludes it.
    
- `dtype` → output array type
    



 Example 1 – Default `num=50`

```python
import numpy as np

x = np.linspace(1, 15)  
print(x)
```

**Output (truncated):**

```
[ 1.          1.28571429  1.57142857 ... 14.71428571 15.        ]
```



 Example 2 – Custom number of points

```python
x = np.linspace(1, 15, 5)
print(x)
```

**Output:**

```
[ 1.   4.5   8.   11.5  15. ]
```



Example 3 – Excluding endpoint

```python
x = np.linspace(1, 15, 5, endpoint=False)
print(x)
```

**Output:**

```
[ 1.   3.8  6.6  9.4  12.2 ]
```


#### Random Array and Random Matrix

We can create arrays filled with random numbers using `np.random.rand()`.  
This generates numbers uniformly between **0 and 1**.

```python
import numpy as np

# 1D Random Array
x = np.random.rand(5)
print(x)
````

**Example Output:**

```
[0.56384794 0.22602629 0.44806777 0.8582133  0.90928771]
```



```python
# 2D Random Matrix (3 rows, 2 columns)
x = np.random.rand(3, 2)
print(x)
```

**Example Output:**

```
[[0.74546836 0.5458692 ]
 [0.62195462 0.68303793]
 [0.91000309 0.37147617]]
```



```python
# 3D Random Matrix (2 × 2 × 3)
x = np.random.rand(2, 2, 3)
print(x)
```

**Example Output:**

```
[[[0.35227776 0.82447525 0.47650569]
  [0.78307986 0.54879399 0.49727558]]

 [[0.14251969 0.69900492 0.55307993]
  [0.54046539 0.42103462 0.88256667]]]
```



#### Computing Trace of a Matrix

The **trace** of a matrix is the sum of its diagonal elements.

Example:
```python
import numpy as np  

a = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9]])

print("Given matrix is:\n", a)

# Using .trace()
print("Trace of the given matrix is:", a.trace())

# Equivalent: using diagonal() + sum()
print("Trace:", sum(a.diagonal()))
````

 Output:

```
Given matrix is:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

Trace of the given matrix is: 15  
Trace: 15
```


#### Finding Transpose of a Matrix

The **transpose** of a matrix is obtained by swapping its rows with columns.

Example:
```python
import numpy as np

a = np.array([[3, 4],
              [5, 6]])

print("Matrix a is:\n", a)

# Using np.transpose()
print("Transpose of matrix a is:\n", np.transpose(a))

# Using .T attribute
print("Transpose of matrix a is:\n", a.T)

# Using .transpose() method
print("Transpose of matrix a is:\n", a.transpose())
````

Output:

```
Matrix a is:
[[3 4]
 [5 6]]

Transpose of matrix a is:
[[3 5]
 [4 6]]

Transpose of matrix a is:
[[3 5]
 [4 6]]

Transpose of matrix a is:
[[3 5]
 [4 6]]
```

### PRACTICE QUESTIONS

1.Compare and analyze memory usage and speed of NumPy arrays vs Python lists with a simple numerical example.

2.Analyze a given array and write Python code to:

3.Replace all elements greater than 10 with 10

4.Find the number of elements equal to 10

5.Write a code to perform the following:

6.Create two 1D arrays a and b

7.Add, subtract, and multiply them element-wise

8.Explain what happens when shapes are incompatible

9.Analyze the effect of broadcasting in array operations with an example involving 2D and 1D arrays.

10.Describe how different data types (int, float, bool) behave in NumPy and how to check them.


---

<p align="center" style="font-size:24px"><b>PANDAS</b></p>



### What is Pandas?

- **Pandas** is an open-source Python library for data manipulation and analysis.  
- Built on top of **NumPy**.  
- Provides two main data structures:  
  - **Series** → One-dimensional labeled array  
  - **DataFrame** → Two-dimensional labeled data structure (like a table)  
- Used for **cleaning, transforming, and analyzing** datasets.  
- Integrates with other libraries like **NumPy** and **Matplotlib**.

#### What is a DataFrame?
- A DataFrame is like a table or spreadsheet.  
- Contains:  
  - Rows and columns  
  - Labeled axes (row index and column names)  
- Supports mixed data types.  


### Pandas Basic Operations

Pandas simplifies data handling by enabling efficient preprocessing, cleaning, transformation, and visualization.

1. **Data Preprocessing**  
   - Handle missing values  

2. **Data Cleaning**  
   - Remove duplicates  
   - Fix inconsistencies  

3. **Data Transformation**  
   - Filter, sort, and modify data  

4. **Data Visualization**  
   - Represent data graphically  

#### Data Preprocessing  

Handle missing values and clean raw data for analysis.  

**Before (Table with Missing Values):**  

| Name  | Age | Salary_Amount |
|-------|-----|---------------|
| Aryan | 25  | NaN           |
| Harsh | NaN | 5000          |
| Kunal | NaN | 7000          |

**After (Missing Values Filled):**  

| Name  | Age | Salary_Amount |
|-------|-----|---------------|
| Aryan | 25  | 6000          |
| Harsh | 28  | 5000          |
| Kunal | 30  | 7000          |

**Operation Applied:**  
Filled missing values with `0` or average using `fillna()`.  

#### Data Cleaning  

Remove duplicates and fix column inconsistencies.  

**Before (Duplicate Rows & Bad Column Names):**  

| Name   | Age | Salary_Amount |
|--------|-----|---------------|
| Aryan  | 25  | 5000          |
| Ha rsh | 25  | 7000          |
| Aryan  | 25  | 5000          |

**After (Cleaned Data):**  

| Name  | Age | Salary |
|-------|-----|--------|
| Aryan | 25  | 5000   |
| Harsh | 25  | 7000   |

**Operation Applied:**  
Removed duplicates using `drop_duplicates()`,  
renamed columns using `rename()`.

#### Data Transformation  

Filter and sort data for better insights.  

**Before (Unfiltered Data):**  

| Product    | Category     | Price |
|------------|-------------|-------|
| Laptop     | Electronics | 800   |
| Hoodie     | Clothing    | 20    |
| Smartphone | Electronics | 1000  |
| Jeans      | Clothing    | 40    |

**After (Filtered & Sorted Data):**  

| Product    | Category     | Price |
|------------|-------------|-------|
| Laptop     | Electronics | 800   |
| Smartphone | Electronics | 1000  |

**Operation Applied:**  
Filtered *Electronics* using `query()`,  
sorted by price using `sort_values()`. 

#### Data Visualization  

Represent data graphically for insights.  

**Before (Raw Data Table):**  

| Category    | Price |
|-------------|-------|
| Electronics | 5000  |
| Clothing    | 2000  |
| Furniture   | 3000  |

**After (Bar Chart Visual):**  

  ![[Pasted image 20250816193724.png]]

**Operation Applied:**  
Plotted sales data using `matplotlib`.  

#### Pandas DataFrame  

A **Pandas DataFrame** is a two-dimensional table-like structure in Python where data is arranged in rows and columns.  
It is widely used for organizing, analyzing, and manipulating data, and can hold different types (numbers, text, dates) across columns.  

**Main parts of a DataFrame:**  
- **Data:** Actual values in the table.  
- **Rows:** Labels that identify each row.  
- **Columns:** Labels that define each data category.  

**Example DataFrame:**  


![[Pasted image 20250816193552.png]]

#### Creating a Pandas DataFrame

Pandas allows us to create a **DataFrame** from many data sources. We can create DataFrames directly from **Python objects** like lists and dictionaries or by reading data from external files like **CSV, Excel, or SQL databases**.



#### 1. Creating DataFrame using a List

If we have a simple list of data, we can easily create a **DataFrame** by passing that list to the `pd.DataFrame()` function.

```python
import pandas as pd  

lst = ['klu', 'is', 'Good', 'for', 'education', 'for', 'green campus']  

df = pd.DataFrame(lst)  
print(df)
```



#### Output

||0|
|---|---|
|0|klu|
|1|is|
|2|Good|
|3|for|
|4|education|
|5|for|
|6|green campus|


#### Creating a DataFrame from a List of Lists

```python
import pandas as pd

data = [['Alice', 25], ['Bob', 30], ['Charlie', 35]]
df = pd.DataFrame(data, columns=['Name', 'Age'])
print(df)
````

**Output:**

```
      Name  Age
0    Alice   25
1      Bob   30
2  Charlie   35
```


#### Creating a DataFrame from a List of Dictionaries

```python
import pandas as pd

data = [
    {'Name': 'Alice', 'Age': 25},
    {'Name': 'Bob', 'Age': 30, 'City': 'London'}
]

df = pd.DataFrame(data)
print(df)
```

**Output (missing values filled with NaN):**

```
   Name  Age   City
0  Alice   25    NaN
1    Bob   30  London
```



#### Pandas `read_csv()` in Python

CSV (**Comma Separated Values**) files are used to store **tabular data**. Pandas provides the `read_csv()` function to load CSV files into a **DataFrame**, which is a powerful structure for data manipulation and analysis.



Example – Using `pandas.read_csv()`

```python
import pandas as pd  

# Read CSV file into a DataFrame
df = pd.read_csv("students.csv")  

# Display first 5 rows
print(df.head())
```



 Explanation

- **`df`** → the DataFrame that stores the loaded data.
    
- **`'students.csv'`** → path to the CSV file you want to read.
    
- **`df.head()`** → shows the first 5 rows of the DataFrame.



#### Creating a DataFrame from a Dictionary

```python
import pandas as pd

data = {
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'City': ['New York', 'London', 'Paris']
}

df = pd.DataFrame(data)
print(df)
````

**Output:**

```
      Name  Age      City
0    Alice   25  New York
1      Bob   30   London
2  Charlie   35    Paris
```



#### Reading a CSV File with Custom Options

You can customize how Pandas reads a CSV file using parameters like `index_col` and `usecols`.

 Example – Using `index_col` and `usecols`

```python
import pandas as pd  

df = pd.read_csv(
    "students.csv",
    index_col="ID",               # Use 'ID' column as index
    usecols=["ID", "Name", "Marks"]  # Only read specific columns
)  

print(df.head())
```



 Explanation

- **`index_col="ID"`** → sets the `"ID"` column as the DataFrame index.
    
- **`usecols=["ID", "Name", "Marks"]`** → loads only these columns from the CSV file.
    



#### Handling Missing Data in Pandas

You can deal with missing values using functions like `dropna()` and `fillna()`.

 Example:

```python
import pandas as pd  

 Example DataFrame with missing values
data = {
    "Name": ["Alex", "Bob", "Charlie", None],
    "Marks": [85, None, 92, 88]
}  

df = pd.DataFrame(data)  

# Drop rows with missing values
df_drop = df.dropna()  

# Fill missing values
df_fill = df.fillna({"Name": "Unknown", "Marks": 0})  

print("Original:\n", df)
print("\nAfter dropna:\n", df_drop)
print("\nAfter fillna:\n", df_fill)
```



#### Reading CSV Directly from a URL

Pandas can read data from online sources if the file is accessible via a direct link.

```python
import pandas as pd  

url = "https://example.com/data.csv"  
df = pd.read_csv(url)  

print(df.head())
```


#### Read Specific Columns using `read_csv`

The `usecols` parameter allows loading **only specific columns** from a CSV file.  
This saves memory and speeds up processing by importing only required data.

 Example:

```python
import pandas as pd  

df = pd.read_csv("students.csv", usecols=["First Name", "Email"])  
print(df)
```

 Output:

||First Name|Email|
|---|---|---|
|1|Shelby|[elijahS7@example.net](mailto:elijahS7@example.net)|
|2|Phillip|[bethany14@example.com](mailto:bethany14@example.com)|
|3|Kristine|[bthompson@example.com](mailto:bthompson@example.com)|
|4|Yesenia|[kaitlinkaiser@example.com](mailto:kaitlinkaiser@example.com)|
|5|Lori|[buchananmanuel@example.net](mailto:buchananmanuel@example.net)|


#### Setting an Index Column using `read_csv`

The `index_col` parameter sets one or more columns as the **DataFrame index**,  
making the specified column(s) act as row labels for easier referencing.

 Example:

```python
import pandas as pd  

df = pd.read_csv("students.csv", index_col="First Name")  
print(df)
```

 Output:

|First Name|Last Name|Sex|Email|Date of birth|Job Title|
|---|---|---|---|---|---|
|Shelby|Summers|Male|[elijah57@example.net](mailto:elijah57@example.net)|1945-10-26|Games developer|
|Phillip|Travis|Male|[bethany14@example.com](mailto:bethany14@example.com)|1910-03-24|Phytotherapist|
|Kristine|Martinez|Female|[bthompson@example.com](mailto:bthompson@example.com)|1992-07-08|Homeopath|
|Yesenia|Todd|Male|[kaitlinkaiser@example.com](mailto:kaitlinkaiser@example.com)|2017-08-03|Market researcher|
|Terrell|Buchanan|Male|[buchananmanuel@example.net](mailto:buchananmanuel@example.net)|1938-12-01|Veterinary surgeon|

#### handling missing values (`na_values`) and reading CSV files with different delimiters (`sep`):

```python
import pandas as pd

# Example 1: Handling Missing Values
df_missing = pd.read_csv(
    "students.csv",
    na_values=["N/A", "Unknown"]  # Replace these strings with NaN
)
print("=== Handling Missing Values ===")
print(df_missing)

# Example 2: Reading CSV with Different Delimiters
# Sample data with mixed delimiters
data = """totalbill;tip;sex;smoker;day;time;size
16.99;1.01;Female;No;Sun;Dinner;2
10.34;1.66;Male;No;Sun;Dinner;3
21.01;3.50;Male;No;Sun;Dinner;3
23.68;3.31;Male;No;Sun;Dinner;2
24.59;3.61;Female;No;Sun;Dinner;4
25.29;4.71;Male;No;Sun;Dinner;4
"""

# Save the data to a CSV file
with open("sample.csv", "w") as file:
    file.write(data)

# Read CSV with custom delimiter (;)
df_delim = pd.read_csv("sample.csv", sep=";")

print("\n=== Reading with Custom Delimiter ===")
print(df_delim)
```

### Output (cleaned):

**Handling Missing Values:**

```
      ID      Name  Marks
0   101    Shelby   88.0
1   102   Phillip   NaN      <- "Unknown" replaced with NaN
2   103  Kristine   91.0
3   104   Yesenia   NaN      <- "N/A" replaced with NaN
```

**Custom Delimiter Read:**

```
   totalbill   tip     sex smoker  day    time  size
0      16.99  1.01  Female     No  Sun  Dinner     2
1      10.34  1.66    Male     No  Sun  Dinner     3
2      21.01  3.50    Male     No  Sun  Dinner     3
3      23.68  3.31    Male     No  Sun  Dinner     2
4      24.59  3.61  Female     No  Sun  Dinner     4
5      25.29  4.71    Male     No  Sun  Dinner     4
```

 Key points:

- Use **`na_values`** to replace strings with `NaN`.
    
- Use **`sep=";"`** (or any other symbol like `:` or `|`) when reading CSVs with non-comma delimiters.

Here’s a **clean Markdown version** of your notes with proper syntax:


#### Basic Operations

#### Adding a Column
```python
df['Salary'] = [50000, 60000, 55000]
````

#### Deleting a Column

```python
df.drop('Salary', axis=1, inplace=True)
```

#### Filtering Data

```python
df[df['Age'] > 28]
```



### Viewing Data

```python
df.head()      # First 5 rows
df.tail()      # Last 5 rows
df.info()      # Summary of DataFrame
df.describe()  # Statistics for numeric columns
```



### Accessing Columns and Rows

#### Access a Column

```python
df['Name']
```

#### Access Multiple Columns

```python
df[['Name', 'City']]
```

#### Access Rows

```python
df.loc[1]   # label-based
df.iloc[1]  # position-based
```

### Modifying Data

#### Updating Values
```python
df.at[0, 'Age'] = 26
````

#### Renaming Columns

```python
df.rename(columns={'Age': 'Years'}, inplace=True)
```



### 👉 [CLICK ME FOR MORE PANDAS — *requires Jupyter installed*](introduction-to-pandas.ipynb)


---
### Data Objects and Attribute Types

**Data** are raw alphanumeric values or raw facts obtained through different acquisition methods.  
**Data objects** are collections of attributes or features that represent real-world entities.  

### Types of Data:
1. Record Data  
2. Transaction Data  
3. Graph Data (Network Data)  
4. Spatial Data  
5. Time-Series Data  
6. Text Data  
7. Sequence Data  
8. Hierarchical Data  


### Record Data  
A collection of records, each with the same set of attributes.  

- **Examples:** Relational databases, Excel spreadsheets  
- **Format:** Rows = objects, Columns = attributes  

**Example Table:**  

| Name  | Age | Dept |
|-------|-----|------|
| Alice | 20  | CSE  |
| Bob   | 22  | ECE  |

### Transaction Data
A set of transactions where each transaction is a set of items.

- **Examples:** Market basket data, Online purchase logs  

**Example Table:**

| TID | Items                |
|-----|---------------------|
| 1   | Milk, Bread, Butter |
| 2   | Bread, Eggs         |
| 3   | Milk, Eggs, Diaper  |

### Graph Data (Network Data)
Consists of **nodes** (objects) and **edges** (relationships).

- **Used for:** Social networks, web analysis, communication networks  
- **Example (Social Network):**  
  - Nodes: People (e.g., Alice, Bob, Carol)  
  - Edges: Friendships (e.g., Alice ↔ Bob, Bob ↔ Carol)



### Spatial Data
Objects have **spatial locations** (coordinates).

- **Use Case:** Google Maps, satellite imagery, urban planning  
- **Used for:** Maps, GPS, GIS (Geographic Information Systems)  
- **Example:**  
  - A point: `(latitude, longitude)` → `(17.385, 78.4867)`  
  - A region: polygon boundaries of a city

#### Categorical Data
**Definition:** Data that represents categories or groups.  
**Characteristics:**  
- No inherent numeric meaning  
- Arithmetic operations are not meaningful  
**Examples:**  
- Gender (Male, Female)  
- Color (Red, Blue, Green)  
- Department (HR, Sales, IT)  

#### Numerical Data
**Definition:** Data that consists of numbers and can be measured.  
**Types:**  
- **Discrete:** Countable values (e.g., number of children)  
- **Continuous:** Measurable quantities (e.g., height, weight)  
**Examples:**  
- Age = 25  
- Salary = $50,000  

#### Ordinal Data
**Definition:** Categorical data with a clear ordering or ranking.  
**Characteristics:**  
- Order matters, but the difference between values is not meaningful  
**Examples:**  
- T-shirt size (Small < Medium < Large)  
- Customer satisfaction (Poor < Fair < Good < Excellent)  

#### Binary Data
**Definition:** Data with only two possible values.  
**Types:**  
- **Symmetric:** Both outcomes are equally important (e.g., Male/Female)  
- **Asymmetric:** One outcome is more important (e.g., Disease: Yes/No)  
**Examples:**  
- Yes/No  
- True/False  


```mermaid
graph TD
    A["Types of Data"] --> B["Categorical / Qualitative"]
    A --> C["Numerical / Quantitative"]

    B --> B1["Nominal: Letters, Hair Color, Symbols, Words"]
    B --> B2["Ordinal: Opinion - Agree, Mostly Agree, Neutral, Mostly Disagree, Disagree"]
    B --> B3["Interval: Time of Day - Morning, Noon, Night"]
    B --> B4["Ordinal: Tumour Grade"]

    C --> C1["Discrete / Interval: Dates 200,1000,1500; Temperature 30°C,45°C,60°C; Other 1.2,4.5,7.2"]
    C --> C2["Continuous / Ratio: Temperature Range, Distance Travelled, Time Interval, Age Range"]

    %% Class definitions
    classDef mainNode fill:#89CFF0,stroke:#333,stroke-width:2px,font-size:20px;
    classDef categorical fill:#FFB6C1,stroke:#333,stroke-width:1.5px,font-size:16px;
    classDef numerical fill:#90EE90,stroke:#333,stroke-width:1.5px,font-size:16px;

    %% Assign classes
    class A mainNode;
    class B,B1,B2,B3,B4 categorical;
    class C,C1,C2 numerical;

```
if you cant see that

![[Pasted image 20250816200822.png]]


###Attributes & its Types

#### Attribute (or dimensions, features, variables)
**Definition:** A data field representing a characteristic or feature of a data object  
**Examples:** Customer ID, name, address  

#### Nominal Attributes
**Definition:** Names or labels with no ordering  
**Operations:** Equality comparison  
**Examples:** Colors, ZIP codes  

#### Ordinal Attributes
**Definition:** Ordered categories  
**Operations:** Comparisons such as `<`, `>`  
**Examples:** Rankings, grades  

#### Interval Attributes
**Definition:** Ordered, and differences are meaningful. No true zero  
**Operations:** `+`, `−`  
**Examples:** Temperature in Celsius  

#### Ratio Attributes
**Definition:** Ordered, differences and ratios are meaningful. Has a true zero  
**Operations:** All arithmetic operations  
**Examples:** Age, weight, height, salary  


### Introduction to Similarity and Dissimilarity

#### Similarity
Numerical measure of how alike two data objects are.  
- Value is higher when objects are more alike.  
- Often falls in the range \([0, 1]\).

**What it is:** A number that tells us how much two data points are alike.  
- Higher value → more alike.  
- Usually ranges from 0 (not alike) to 1 (exactly alike).

#### Dissimilarity (e.g., distance)
Numerical measure of how different two data objects are.  
- Lower when objects are more alike.  
- Minimum dissimilarity is often 0.  
- Upper limit varies.

**What it is:** A number that tells us how different two data points are.  
- Lower value → more alike.  
- Minimum = 0 (identical points).  
- Maximum can vary.

**Proximity** refers to a similarity or dissimilarity.
**Proximity** is just a general term for either similarity or dissimilarity.

---

#### Data matrix
Let there be \(n\) data points with \(p\) dimensions. Two modes:

Imagine a table where each **row** is a data point (like a person) and each **column** is a feature (like age or height).  

- **n** = number of data points (rows)  
- **p** = number of features (columns)  

**1. Multi-dimensional representation (full data matrix)**  
$$
X = 
\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1p} \\
x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \dots & x_{np} \\
\end{bmatrix}
$$

- Each$$ x_{ij}$$ is the value of feature \(j\) for data point \(i\).

**2. Dissimilarity matrix**  
- \(n\) data points, but registers only the distance between points.  
- Often stored as a triangular matrix:  

Instead of keeping all features, sometimes we just care about **distances between data points**.  

- Only stores how far apart points are.  
- Often stored as a **triangular matrix** (because distance from A→B is same as B→A):


$$
D = 
\begin{bmatrix}
0 & d_{12} & \dots & d_{1n} \\
d_{21} & 0 & \dots & d_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
d_{n1} & d_{n2} & \dots & 0 \\
\end{bmatrix}
$$
- Single mode: \(d(n,1)\)

- Each $$ d_{ij} $$ = distance between point i and point j. 
- This is useful when we want to see which points are close or far from each other.


#### Binary Attributes
- Binary attributes are features that **only have 2 possible values**.  
  Example: Yes/No, True/False, Male/Female.

---

#### Method 1: Simple Matching
- Used for **binary attributes**.  
- Count how many attributes **match** between two data points.  
- Formula:

$$
\text{Similarity} = \frac{m}{p}
$$

Where:  
- \(m\) = number of matches  
- \(p\) = total number of attributes  

- Can also generalize this to attributes with **more than 2 states** (e.g., colors: red, yellow, blue, green).

---

#### Method 2: Convert Nominal Attributes to Binary
- If an attribute has multiple categories, create **one binary attribute for each category**.  
- Example:  
  Original attribute: Color = {Red, Yellow, Blue}  
  Converted to binary attributes:  
  - IsRed? (0/1)  
  - IsYellow? (0/1)  
  - IsBlue? (0/1)  
- Then, simple matching can be applied to these binary attributes.

$$d(i,j)=(p-m)/p$$

#### Contingency Table for Binary Data

A contingency table summarizes how two objects, **i** and **j**, compare across binary attributes (0 or 1):

|           | Object j = 1 | Object j = 0 | Row Sum |
|-----------|--------------|--------------|---------|
| Object i = 1 | q          | r            | q + r   |
| Object i = 0 | s          | t            | s + t   |
| Column Sum   | q + s      | r + t        | p       |

Where:  
- **q** = number of attributes where both i and j are 1  
- **r** = number of attributes where i = 1, j = 0  
- **s** = number of attributes where i = 0, j = 1  
- **t** = number of attributes where both are 0  
- **p** = total number of attributes  

---

#### 1. Distance Measure for Symmetric Binary Variables

For symmetric binary variables, both 1s and 0s matter equally. The distance between objects i and j is:

$$
d(i,j) = \frac{r + s}{q + r + s + t}
$$

Explanation:  
- **Numerator (r + s):** Number of mismatches between i and j  
- **Denominator (q + r + s + t):** Total number of attributes  

> Higher distance → more dissimilar objects.  
> Lower distance → more similar objects.  

---

#### 2. Distance Measure for Asymmetric Binary Variables

For asymmetric binary variables, only the presence (1) matters. Absences (0) are ignored:

$$
d(i,j) = \frac{r + s}{q + r + s}
$$

Explanation:  
- **Numerator (r + s):** Number of mismatches where at least one object has 1  
- **Denominator (q + r + s):** Only consider attributes where at least one object has 1  

---

#### 3. Jaccard Coefficient (Similarity Measure for Asymmetric Binary Variables)

The **Jaccard coefficient** measures similarity rather than distance:

$$
\text{Sim}_{\text{Jaccard}}(i, j) = \frac{q}{q + r + s}
$$

Explanation:  
- **Numerator (q):** Number of attributes where both i and j are 1  
- **Denominator (q + r + s):** Attributes where at least one object has 1  

> Value ranges from 0 (completely different) to 1 (identical).  

---

#### 4. Coherence

The Jaccard coefficient is also called **coherence**:

$$
\text{coherence}(i, j) = \frac{\text{sup}(i,j)}{\text{sup}(i) + \text{sup}(j) - \text{sup}(i,j)} = \frac{q}{(q+r) + (q+s) - q} = \frac{q}{q + r + s}
$$

Where:  
- **sup(i,j) = q** = number of positive matches  
- **sup(i) = q + r** = total positives in object i  
- **sup(j) = q + s** = total positives in object j  

> Coherence = Jaccard similarity.

---

### Dissimilarity Between Binary Variables

We calculate dissimilarity between objects when attributes are binary, considering **symmetric** and **asymmetric** cases.

---

#### 1. Data Example

| Name | Gender | Fever | Cough | Test-1 | Test-2 | Test-3 | Test-4 |
|------|--------|-------|-------|--------|--------|--------|--------|
| Jack | M      | N     | N     | N      | N      | N      | N      |
| Mary | F      | N     | N     | N      | N      | N      | N      |
| Jim  | M      | N     | N     | N      | N      | N      | N      |

- **Symmetric attribute:** `Gender` (both outcomes are equally important)  
- **Asymmetric attributes:** `Fever`, `Cough`, `Test-1` … `Test-4` (presence matters, absence does not)

##### Identifying Symmetric vs Asymmetric Binary Attributes

| Attribute Type | Definition | Count 0? | Example |
|----------------|-----------|----------|---------|
| Symmetric      | Both outcomes are equally meaningful | Yes | Gender (M/F), Yes/No survey response |
| Asymmetric     | Only one outcome is meaningful; 0 carries little/no info | No (ignore 0-0 matches) | Fever presence, Positive test result |

**Rule of Thumb:**  
- Ask: “Does absence (0) carry information?”  
  - Yes → Symmetric  
  - No → Asymmetric


Binary encoding:  
- $Y$ or $P$ → 1  
- $N$ → 0

---

### 2. Dissimilarity Formula

#### 2.1 Symmetric Binary Attribute
For objects $i$ and $j$, the dissimilarity is defined as:

$$
d(i, j) = \frac{\text{Number of mismatches}}{\text{Total number of attributes considered}}
$$

#### 2.2 Asymmetric Binary Attribute
For asymmetric attributes, where double-zeros are ignored:

$$
d(i, j) = \frac{\text{Number of mismatches where at least one value = 1}}{\text{Number of attributes where at least one value = 1}}
$$

---

### 3. Example Calculations

#### 3.1 Symmetric Attribute: Gender
- Jack (M) vs Mary (F) → mismatch → $d = 1/1 = 1.0$  
- Jack (M) vs Jim (M) → match → $d = 0/1 = 0.0$  
- Mary (F) vs Jim (M) → mismatch → $d = 1/1 = 1.0$

#### 3.2 Asymmetric Attributes: Fever, Cough, Tests
- All attributes are $N$ (0) → no mismatches counted, denominator = 0 → handle as **0 or undefined**  
- If some values were 1, we would count mismatches where at least one = 1.

**Given example results (approximated from your notes):**

| Pair         | Dissimilarity |
|--------------|---------------|
| Jack–Mary    | 0.33          |
| Jack–Jim     | 0.67          |
| Mary–Jim     | 0.75          |

> These values consider mismatches over asymmetric attributes only.

---

### 4. Notes
- Symmetric attributes contribute equally to dissimilarity.  
- Asymmetric attributes only count when at least one object has a positive value (1).  
- This method is commonly used in clustering and similarity analysis for binary datasets.

---

### Z-Score

A **Z-score** measures how far a raw score is from the mean in units of standard deviation.

---

#### 1. Definition

Let:  
- $X$ = raw score  
- $\mu$ = mean of the population  
- $\sigma$ = standard deviation of the population  

The Z-score is calculated as:

$$
z = \frac{X - \mu}{\sigma}
$$

- **Interpretation:**  
  - $z > 0$: score above the mean  
  - $z < 0$: score below the mean  

It represents the distance of the raw score from the mean in **standard deviation units**.

---

### 2. Alternative:  Z-Score Using  Mean Absolute Deviation

Let:  
- **$n$** is the **total number of observations** for that feature
- $x_{if}$ = raw score of observation $i$ for feature $f$  
- $m_f$ = mean of feature $f$  
- $s_f$ = absolute mean deviation of feature $f$  

---

#### 1. Mean of Feature $f$

$$
m_f = \frac{1}{n} \sum_{i=1}^{n} x_{if}
$$



#### 2. Absolute Mean Deviation (MAD)

$$
s_f = \frac{1}{n} \sum_{i=1}^{n} |x_{if} - m_f|
$$

- Uses absolute differences from the mean  
- More robust to outliers than standard deviation  


#### 3. Z-Score Using MAD

$$
z_{if} = \frac{x_{if} - m_f}{s_f}
$$

- Measures how far $x_{if}$ is from the mean in units of MAD  
- $z_{if} > 0$ → above mean  
- $z_{if} < 0$ → below mean

---

### Data Matrix and Dissimilarity Matrix Example

#### 1. Dissimilarity Matrix (Euclidean Distance)

|       | x1   | x2   | x3   | x4   |
|-------|------|------|------|------|
| x1    | 0    | 3.61 | 2.2  | 4.24 |
| x2    | 3.61 | 0    | 5.1  | 1    |
| x3    | 2.2  | 5.1  | 0    | 5.39 |
| x4    | 4.24 | 1    | 5.39 | 0    |

- Diagonal = 0 (distance of object with itself)  
- Off-diagonal = Euclidean distance between objects  

#### 2. Original Data Matrix (Attributes)

| Object | Attribute 1 | Attribute 2 |
|--------|-------------|-------------|
| x1     | 1           | 2           |
| x2     | 3           | 5           |
| x3     | 2           | 0           |
| x4     | 4           | 5           |

![[Pasted image 20250816205340.png]]


#### 3. Notes

- Euclidean distance formula:

$$
d(x_i, x_j) = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \dots + (x_{ip}-x_{jp})^2}
$$

- $x_i, x_j$ = objects  
- $p$ = number of attributes  
- Dissimilarity matrix is **symmetric**: $d(x_i, x_j) = d(x_j, x_i)$  
- Useful for clustering or similarity analysis

### Minkowski Distance

A **popular distance measure** in multi-dimensional space.

#### 1. Definition

Let:  
- $i = (x_{i1}, x_{i2}, \dots, x_{ip})$  
- $j = (x_{j1}, x_{j2}, \dots, x_{jp})$  

The **Minkowski distance** of order $h$ is:

$$
d(i, j) = \sqrt[h]{\,|x_{i1} - x_{j1}|^h + |x_{i2} - x_{j2}|^h + \dots + |x_{ip} - x_{jp}|^h\,}
$$

<p align="center" style="font-size:18px">(or)</p>


$$
d(i, j) = \lim_{h \to \infty} \left( \sum_{f=1}^{p} |x_{if} - x_{jf}|^h \right)^{\frac{1}{h}} = \max_f |x_{if} - x_{jf}|
$$


- $h$ = order of the distance (also called **L-h norm**)  
- Special cases:  
  - $h = 1$ → **Manhattan distance**  
  - $h = 2$ → **Euclidean distance**  
  - $h \to \infty$ → **supremum distance**

#### 2. Properties

1. **Positive definiteness:**  
   $$
   d(i, j) > 0 \text{ if } i \neq j, \quad d(i, i) = 0
   $$

2. **Symmetry:**  
   $$
   d(i, j) = d(j, i)
   $$

3. **Triangle inequality:**  
   $$
   d(i, j) \le d(i, k) + d(k, j)
   $$

- A distance measure that satisfies these three properties is called a **metric**.
